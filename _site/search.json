[
  {
    "objectID": "content/drafts/ODE-Solver/ODE-solve.html",
    "href": "content/drafts/ODE-Solver/ODE-solve.html",
    "title": "Taylor Series approximations of ODEs: The Algorithm",
    "section": "",
    "text": "If you ever start doing university level maths, physics, chemistry or even economics, you will most likely start running into differential equations. These are equations that describe functions in terms of their derivatives. Normally in these classes, at least at the start, all the problems are solvable by hand with some fairly rudimentary algebra and calculus. However, if you ever try and work with these problems in the real world, you will quickly realise that a lot of differential equations aren’t solvable.\nThis is where numerical methods become useful as they provide one the ability to approximate the solution of the differential equation without explicitly solving it."
  },
  {
    "objectID": "content/drafts/ODE-Solver/ODE-solve.html#eulers-method",
    "href": "content/drafts/ODE-Solver/ODE-solve.html#eulers-method",
    "title": "Taylor Series approximations of ODEs: The Algorithm",
    "section": "Euler’s Method",
    "text": "Euler’s Method\nOne of the first numerical methods that most people get taught is Euler’s method. There are many better writers who can explain this method much better than me, so I won’t go into too much detail. Essentially, this method uses the definition of the differential equation to calculate steps of a given length h. For an ODE (differential equation of one variable) of order n, the method can be described as:\n\\[\n\\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n-1}(x+h)\\\\\n\\end{bmatrix} = \\begin{bmatrix}\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n-1}(x)\\\\\n\\end{bmatrix} + \\begin{bmatrix}\nh & 0 & 0 &  ... & 0\\\\\n0 & h & 0 &  ... & 0\\\\\n0 & 0 & h &  ... & 0\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & h\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\ny'(x)\\\\\ny''(x)\\\\\ny'''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix}\n\\]\nThis can also be written as:\n\\[\n\\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1 & h & 0 &  ... & 0\\\\\n0 & 1 & h &  ... & 0\\\\\n0 & 0 & 1 &  ... & 0\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix}\n\\]\n\nHow to expand the Euler method?\nThe Euler Method approximates functions as straight lines at each point and to then evaluate the function at the next point. However, if the function is not much like a straight line then this approximation will fail to produce accurate results. Therefore, the question becomes how could one take into account the curve of the function between steps."
  },
  {
    "objectID": "content/drafts/ODE-Solver/ODE-solve.html#taylor-series",
    "href": "content/drafts/ODE-Solver/ODE-solve.html#taylor-series",
    "title": "Taylor Series approximations of ODEs: The Algorithm",
    "section": "Taylor Series",
    "text": "Taylor Series\nA solution to the problem posed above is to use a Taylor series of the function, which describes a function as a polynomial determined by its derivatives. Explicitly, it is defined as :\n\\[\ny(x) =  \\sum_{n = 0}^{\\infty}  \\frac{\\mathbb y^{n}(a)}{n!}\\cdot(x - a)^n\n\\]\nWhere \\(a\\) is the point where the approximation is built.\nWhen looking at the point \\(a + h\\) exclusively, this formula transforms to:\n\\[\ny(a+h) = \\sum_{n = 0}^{\\infty}  \\frac{\\mathbb y^{n}(a)}{n!}\\cdot(h)^n\n\\]\nA nice property of the Taylor series is that it has a really simple derivative function:\n\\[ y'(a+h) = \\sum_{n = 0}^{\\infty}  \\frac{\\mathbb y^{n+1}(a)}{n!}\\cdot(h)^n \\] \\[ y^m(a+h) = \\sum_{n = 0}^{\\infty}  \\frac{\\mathbb y^{n+m}(a)}{n!}\\cdot(h)^n \\]\nThis function describes a linear combination of the derivatives of a function, which can then be written as a matrix multiplication\n\\[ \\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^n}{n!}\\\\\n0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{n-1}}{(n-1)!}\\\\\n0 & 0 & 1 &  ... & \\frac{h^{n-2}}{(n-2)!}\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\ny'(x)\\\\\ny''(x)\\\\\ny'''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix}\\]\nOne can see that this step matrix is similar to the matrix that defines the steps of the Euler method, however the Euler method doesn’t take into account each of the higher derivatives, whereas this new step matrix does.\nNow expanding the input vector to contain x and h information;\n\\[ \\begin{bmatrix}\nh\\\\\nx+h\\\\\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 &  ... & 0\\\\\n1 & 1 & 0 & 0 & 0 &  ... & 0\\\\\n0 & 0 & 1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^n}{n!}\\\\\n0 & 0 & 0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{n-1}}{(n-1)!}\\\\\n0 & 0 & 0 & 0 & 1 &  ... & \\frac{h^{n-2}}{(n-2)!}\\\\\n... & ... & ... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nh\\\\\nx\\\\\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix}\\]\nFor the sake of simplicity, this equation can referred to as \\(Y(x+h) = S \\cdot Y(x)\\)"
  },
  {
    "objectID": "content/drafts/ODE-Solver/ODE-solve.html#how-to-make-the-approximation-fit-the-ode",
    "href": "content/drafts/ODE-Solver/ODE-solve.html#how-to-make-the-approximation-fit-the-ode",
    "title": "Taylor Series approximations of ODEs: The Algorithm",
    "section": "How to make the approximation fit the ODE",
    "text": "How to make the approximation fit the ODE\nThe current \\(Y(x+h) = S \\cdot Y(x)\\) describes the path of a polynomial through space. To use it to approximate an ODE, it must be slightly modified such that for each step of the approximation, the condition of the ODE is met. This part of the problem has to be turned into two separate problems, one for ODEs that are exclusively linear combinations of the input vector and one they are not.\n\nCase 1 : Exclusively linear\nFor an ODE that is a linear combination of x, y and y’s derivatives, such as \\(y''=x+y\\), on can simply change multiply the function by a matrix representing this linear combination. This can be written as:\n\\[T \\cdot Y(x+h) = T \\cdot S \\cdot Y(x) \\]\nKnowing that each row of the stepping matrix represents one of h, x, y or y’s derivatives, creating a transformation matrix is fairly easy.\nSticking with the same sample ODE \\(y''=x+y\\), the transformation matrix can be defined as;\n\\[\nT_1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 1 & 1 & 0 & 0\\\\\n\\end{bmatrix}\n\\]\nHowever, thinking of this ODE as \\(x = y'' - y\\) and \\(y = y'' - x\\) it is just as reasonable to define T as:\n\\[\nT_2 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & -1 & 0 & 1\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nor\n\\[\nT_3 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & -1 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nSo what is best option for the approximation. It’s not yet obvious, so lets look at the resulting step matrices from these transformations \\(T \\cdot S\\)\n\\[\nT_1 \\cdot S= \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & h & \\frac{h^2}{2}\\\\\n0 & 0 & 0 & 1 & h\\\\\n1 & 1 & 1 & h & \\frac{h^2}{2}\\\\\n\\end{bmatrix}\n\\]\n\\[\nT_2 \\cdot S= \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 0 &  -1 & -h & 1-\\frac{h^2}{2}\\\\\n0 & 0 & 1 & h & \\frac{h^2}{2}\\\\\n0 & 0 & 0 & 1 & h\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\n\\[\nT_3 \\cdot S= \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n-1 & -1 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1 & h\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nNow we have to look at an important part of approximations, truncation error, that is, assuming the function has a solution \\(y^*\\), what is the magnitude of the difference between this function and the approximation. Formally, this is said to be \\(r = y^* - y\\). Since 5 curves are being simulated at the same time, this remainder still be seen as a vector R with the definition:\n\\[R = Y^*(x+h) - Y(x+h) = Y^*(x+h) - S \\cdot Y(x) \\]\nSince this approximation is based on the Taylor series, the truncation error is the remaining terms in the approximation. For an approximation with the greatest term k, the truncation error is therefore:\n\\[ r = \\sum_{n = k+1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\]\nFor the standard stepping matrix for the order two ODE:\n\\[\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & h & \\frac{h^2}{2}\\\\\n0 & 0 & 0 & 1 & h\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix} \\rightarrow R =  \\begin{bmatrix}\n0\\\\\n0\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\n\\]\nSince this is the error for an un-transformed system, by simply applying the transformation matrix \\(R\\) we can find the truncation error for the transformed system.\n\\[T_1 \\cdot R = \\begin{bmatrix}\n0\\\\\n0\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n0 + \\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0\\\\\n0\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\\]\n\\[T_2 \\cdot R = \\begin{bmatrix}\n0\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n-\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0\\\\\n\\mathbb y'(x)\\cdot(h) + \\frac{\\mathbb y''(x)}{2}\\cdot(h)^2\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\\]\n\\[T_3 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\n0-\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0\\\\\n0\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\\]\nAlthough the magnitude of these errors are theoretically possible to compute, it’s a lot of work considering that the question we are trying to answer is which one is the smallest. One tool we can employ to simplify this problem is big-O notation. Big-O notation is defined as \\(f(x) = O(g(x))\\) as \\(x \\rightarrow a\\) if \\(|f(x)| = M \\cdot g(x)\\) . It describes the upper limit of a function as it’s variable approaches a. In this case, we are interested in the upper limit as h approaches 0.\nApplying this concept to the R vectors:\n\\[T_1 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h^3)\\\\\n\\end{bmatrix}\n\\]\n\\[T_2 \\cdot R = \\begin{bmatrix}\n0\\\\\nO(h)\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n\\]\n\\[T_3 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n\\]\nNow looking at the magnitude of these vectors:\n\\[ |T_1 \\cdot R | = \\sqrt{(O(h^3))^2 + (O(h^2))^2 + (O(h^3))^2} \\] \\[ |T_2 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^3))^2 + (O(h^2))^2 + (O(h))^2} \\] \\[ |T_3 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^2))^2 + (O(h))^2} \\]\nSince \\(f_1 \\cdot f_2 = O(g_1g_2)\\);\n\\[ |T_1 \\cdot R |  = \\sqrt{O(h^6) + O(h^4) + O(h^6)} \\] \\[ |T_2 \\cdot R |  = \\sqrt{O(h^2) + O(h^6) + O(h^4) + O(h^2)} \\] \\[ |T_3 \\cdot R |  = \\sqrt{O(h^2) + O(h^4) + O(h^2)} \\]\nSince \\(f_1 + f_2 = O(max(g_1,g_2))\\);\n\\[ |T_1 \\cdot R | = \\sqrt{O(h^4)} \\] \\[ |T_2 \\cdot R | = \\sqrt{O(h^2)} \\] \\[ |T_3 \\cdot R | = \\sqrt{O(h^2)} \\]\nTherefore:\n\\[ |T_1 \\cdot R | = O(h^2) \\] \\[ |T_2 \\cdot R | = O(h) \\] \\[ |T_3 \\cdot R | = O(h) \\]\nThis shows that \\(|R_1|\\) has the lowest upper limit of the 3 vectors, showing that \\(R_1\\) is the smallest remainder matrix and therefore \\(T_1\\) is the best transformation matrix for approximating the ODE.\nNow looking more generally for an ODE of order k, the stepping matrix, the remainder matrix and the big-O remainder matrix is ;\n\\[\nS = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 &  ... & 0\\\\\n1 & 1 & 0 & 0 & 0 &  ... & 0\\\\\n0 & 0 & 1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^k}{k!}\\\\\n0 & 0 & 0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{k-1}}{(k-1)!}\\\\\n0 & 0 & 0 & 0 & 1 &  ... & \\frac{h^{k-2}}{(k-2)!}\\\\\n... & ... & ... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix}\n\\] \\[\nR =  \\begin{bmatrix}\n0\\\\\n0\\\\\n\\sum_{n = k+1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = k}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = k-1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n...\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\n=  \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^{k+1})\\\\\nO(h^{k})\\\\\nO(h^{k-1})\\\\\n...\\\\\nO(h)\n\\end{bmatrix}\n\\]\n\\[ |R| = O(h) \\]\nTherefore, to determine the best transformation matrix for a given ODE, you want to define the element that gives the \\(O(h)\\) remainder as a combination of smaller remainder terms as this will then convert the approximation to \\(O(h^2)\\). Therefore, to optimise the transformation matrix, wherever possible, define higher order derivatives as functions of lower order derivatives, x and y, and where this is not possible, leave it as is.\n\n\nCase 2: Everything Else\nFor non-linear cases, no transformation matrix will match the conditions set out in the ODE. Therefore, an adjustment function , \\(A(x)\\)has to be defined to match the conditions such that:\n\\[Y(x+h) =  A(S \\cdot Y(x))\\]\nTaking the example ODE \\(Y = xY''\\), we have a similar problem to before where three adjustment functions can be defined such that \\[Y \\leftarrow xY''\\] \\[Y'' \\leftarrow \\frac{Y}{x}\\] \\[x \\leftarrow \\frac{Y}{Y''}\\]\nTo now look at which one gives the best approximation one can take a similar action to previously, by applying the adjustment function to the remainder function\n\\[ A_1(\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\nx_5\\\\\n\\end{bmatrix})\n=\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_2 \\cdot x_5\\\\\nx_4\\\\\nx_5\\\\\n\\end{bmatrix}\n\\]\n\\[ A_2(\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\nx_5\\\\\n\\end{bmatrix})\n=\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\nx_3 / x_2\\\\\n\\end{bmatrix}\n\\forall   x_2 \\neq 0\nor\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\n0\\\\\n\\end{bmatrix}\nx_2 = 0\n\\]\n\\[ A_1(\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\nx_3\\\\\nx_4\\\\\nx_5\\\\\n\\end{bmatrix})\n=\n\\begin{bmatrix}\nx_1\\\\\nx_3/x_5\\\\\nx_3\\\\\nx_4\\\\\nx_5\\\\\n\\end{bmatrix}\n\\forall   x_5 \\neq 0\n\\]\nNow applying these functions to the remainder vector;\n\\[A_1(R) = \\begin{bmatrix}\n0\\\\\n0\\\\\n0\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\n\\]\n\\[A_2(R) =\n\\begin{bmatrix}\n0\\\\\n0\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n0\\\\\n\\end{bmatrix}\n\\]\n\\[A_3(R) = \\begin{bmatrix}\n0\\\\\n\\frac{\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\}{\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\}\\\\\n\\sum_{n = 3}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\sum_{n = 1}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\\\\n\\end{bmatrix}\n\\]\nConverting again to the big-O notation;\n\\[A_1(R) = \\begin{bmatrix}\n0\\\\\n0\\\\\n0\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n\\]\n\\[A_2(R) =\n\\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\n0\\\\\n\\end{bmatrix}\n\\]\n\\[A_3(R) = \\begin{bmatrix}\n0\\\\\nO(h^2)\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n\\]\nAnd then taking the magnitude:\n\\[ |A_1(R)| = O(h) \\] \\[ |A_2(R)| = O(h^2) \\] \\[ |A_3(R)| = O(h) \\]\nTherefore, \\(A_2(x)\\) is the best adjustment function for this ODE. Unlike the exclusively linear case there is no hard and fast rule to get the best adjustment function, however similarly to the linear case you want to remove the highest order term, but the adjustment functions will need to be checked to determine the best one."
  },
  {
    "objectID": "content/drafts/ODE-Solver/ODE-solve.html#the-final-algorithm",
    "href": "content/drafts/ODE-Solver/ODE-solve.html#the-final-algorithm",
    "title": "Taylor Series approximations of ODEs: The Algorithm",
    "section": "The Final Algorithm",
    "text": "The Final Algorithm\n\nLinear Case\n\nFor an ODE of order k, and a step size h convert the initial condition into a vector Y(x) such that: \\[Y(x) = \\begin{bmatrix}\nh\\\\\nx\\\\\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{k}(x)\\\\\n\\end{bmatrix}\\]\nDefine an initial stepping function such that: \\[ \\begin{bmatrix}\nh\\\\\nx+h\\\\\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 &  ... & 0\\\\\n1 & 1 & 0 & 0 & 0 &  ... & 0\\\\\n0 & 0 & 1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^n}{n!}\\\\\n0 & 0 & 0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{n-1}}{(n-1)!}\\\\\n0 & 0 & 0 & 0 & 1 &  ... & \\frac{h^{n-2}}{(n-2)!}\\\\\n... & ... & ... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nh\\\\\nx\\\\\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix}\\]\nDefine a transformation matrix to apply to Y(x) such that Y(x) always satisfies the conditions of the ODE, and that minimises truncation error.\nDefine the new stepping function such that: \\[ \\begin{bmatrix}\nh\\\\\nx+h\\\\\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} = T \\cdot \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 &  ... & 0\\\\\n1 & 1 & 0 & 0 & 0 &  ... & 0\\\\\n0 & 0 & 1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^n}{n!}\\\\\n0 & 0 & 0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{n-1}}{(n-1)!}\\\\\n0 & 0 & 0 & 0 & 1 &  ... & \\frac{h^{n-2}}{(n-2)!}\\\\\n... & ... & ... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nh\\\\\nx\\\\\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix}\\]\nIterate this stepping function over the desired domain.\n\n\n\nGeneral case\n\nSame as the linear case\nSame as linear case\nDefine a transformation function to apply to Y(x) such that Y(x) always satisfies the conditions of the ODE, and that minimises truncation error.\nDefine a new stepping matrix such that: \\[ \\begin{bmatrix}\nh\\\\\nx+h\\\\\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} = A(\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 &  ... & 0\\\\\n1 & 1 & 0 & 0 & 0 &  ... & 0\\\\\n0 & 0 & 1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^n}{n!}\\\\\n0 & 0 & 0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{n-1}}{(n-1)!}\\\\\n0 & 0 & 0 & 0 & 1 &  ... & \\frac{h^{n-2}}{(n-2)!}\\\\\n... & ... & ... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\nh\\\\\nx\\\\\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix})\\]\nIterate this stepping function over the desired domain."
  },
  {
    "objectID": "content/post/taylor-series/post-3.html",
    "href": "content/post/taylor-series/post-3.html",
    "title": "Making my ODE solver solve ODEs",
    "section": "",
    "text": "After writing out the last post where I wrote out a python library for using an improved version of Euler’s method to solve ODEs. But so far, we haven’t been solving ODES, instead we have just been taking an initial value and iterating it over the length of a domain. To To make the ODE estimator work, we need to ensure that the conditions of the ODE are met at each step."
  },
  {
    "objectID": "content/post/taylor-series/post-3.html#but-why-does-this-matter",
    "href": "content/post/taylor-series/post-3.html#but-why-does-this-matter",
    "title": "Making my ODE solver solve ODEs",
    "section": "But Why Does This Matter",
    "text": "But Why Does This Matter\nThe reason that we want to reframe ODEs in this way is because of the following fact:\nFor all constant-linear ODEs, we can express the ODE as a matrix such that applying it to any point in the vector space would map any point to a valid point on the curve defined by the ODE\n\nLooking at the equations above, these matrices (\\(T\\)) are:\n\nSimple Harmonic Motion: \\[T = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & -\\omega^2 & 0 & 0\\\\\n\\end{bmatrix}\n\\]\nRadioactive Decay: \\[T = \\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & -\\lambda & 0\\\\\n\\end{bmatrix}\\]\nRC Circuit Equation: \\[T = \\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & \\frac{-1}{RC} & 0\\\\\n\\end{bmatrix}\\]\nDamped Harmonic Oscillator: \\[T = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & -\\omega^2 & -2\\gamma & 0\\\\\n\\end{bmatrix}\\]\nHeat Equation (One-Dimensional): \\[T = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & \\frac{1}{\\alpha} & 0\\\\\n\\end{bmatrix}\\]\nExponential Growth or Decay: \\[T = \\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0\\\\\n0 & 0 & 1 & 0\\\\\n0 & 0 & k & 0\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "content/post/taylor-series/post-3.html#using-these-to-fit-odes",
    "href": "content/post/taylor-series/post-3.html#using-these-to-fit-odes",
    "title": "Making my ODE solver solve ODEs",
    "section": "Using these to fit ODEs",
    "text": "Using these to fit ODEs\nNow that we can express the ODEs in the form of a matrix, we can implement these matriexies in the ODE solver package to make the solution fit the ode. It’s important here to note that I’ve diverted from my old definitions of \\(Y\\) here, where the first element of the vector is \\(y(x)\\).\nTo make a step in the approximation we use the following equation:\n\\[ \\begin{bmatrix}\n1 \\\\\nx+h \\\\\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} =  S \\cdot \\begin{bmatrix}\n1 \\\\\nx\\\\\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix}\n\\epsilon \\]\nWhere \\(S\\) is: \\[ \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & ... & 0 \\\\\nh & 1 & 0 & 0 & 0 & ... & 0 \\\\\n0 & 0 & 1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^n}{n!}\\\\\n0 & 0 & 0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{n-1}}{(n-1)!}\\\\\n0 & 0 & 0 & 0 & 1 &  ... & \\frac{h^{n-2}}{(n-2)!}\\\\\n... & ... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix}\\]\nWhen making this step, the error in the approximation will move the point away from the plane that contains all valid solutions to the ODE, and therefore we will have to snap it back using one of the transformation matrices (\\(T\\)).\nImplementing this method in our python library:\n\ndef expanded_euler(dims, h):\n    step_matrix = np.zeros((dims, dims))\n    for i in range(dims):\n        for j in range(i, dims):\n            # Is 1, and h at j-i =0, 1 respectively\n            step_matrix[i, j] = h ** (j - i) / math.factorial(j - i)\n    expanded_matrix = add_x_and_1(step_matrix, h)\n    return expanded_matrix\n\n\ndef add_x_and_1(original_matrix, h):\n    new_size = len(original_matrix) + 2\n    new_matrix = np.zeros((new_size, new_size), dtype=original_matrix.dtype)\n\n    # Set the 2x2 top left matrix\n    new_matrix[0:2, 0:2] = [[1, 0], [h, 1]]\n\n    # Copy the original matrix to the bottom right of the new matrix.\n    new_matrix[2:, 2:] = original_matrix\n    return new_matrix\n\n\ndef linear(y, step_matrix_generator, transformation_matrix, steps=10, h=0.1):\n    dims = len(y) - 2\n    step_matrix = transformation_matrix @ step_matrix_generator(dims, h)\n    output_list = []\n\n    y_n = y.copy()\n    i = 0\n    while i &lt; steps:\n        y_n = step_matrix @ y_n\n        output_list.append(y_n)\n        i += 1\n\nBind this machinery together, and you get a tool capable of tackling the initial example of \\(y' = 2x\\) passing through the point (0,0):\n\nimport numpy as np\nimport math\n\n\nclass Solution:\n    def __init__(self, input_list: list):\n        solution_list = sorted(input_list, key=lambda x: x[1])\n\n        dims = len(solution_list[0]) - 2\n        self.x = np.array([x[1] for x in input_list])\n\n        value_lists = [[] for _ in range(dims)]\n\n        for v in input_list:\n            for i in range(dims):\n                value_lists[i].append(v[i + 2])\n\n        for i in range(dims):\n            self.__dict__[f\"y_{i}\"] = np.array(value_lists[i])\n\n    def interpolate(self, x, y_n):\n        \"\"\"\n        allows you to get any value from the solution by interpolating the points\n\n        \"\"\"\n        y_values = self.__dict__[f\"y_{y_n}\"]\n\n        x_max_index = np.where(self.x &gt;= x)[0][0]\n        x_min_index = np.where(self.x &lt;= x)[0][-1]\n\n        x_at_x_max = self.x[x_max_index]\n        x_at_x_min = self.x[x_min_index]\n\n        y_at_x_max = y_values[x_max_index]\n        y_at_x_min = y_values[x_min_index]\n\n        slope = (y_at_x_max - y_at_x_min) / (x_at_x_max - x_at_x_min)\n\n        value = y_at_x_min + slope * (x - x_at_x_min)\n        return value\n\ndef linear(y, step_matrix_generator, transformation_matrix, steps=10, h=0.1):\n    dims = len(y) - 2\n    step_matrix = transformation_matrix @ step_matrix_generator(dims, h)\n    output_list = []\n\n    y_n = y.copy()\n    i = 0\n    while i &lt; steps:\n        y_n = step_matrix @ y_n\n        output_list.append(y_n)\n        i += 1\n\n    return Solution(output_list)\n\n\ninit_y = [1,0,0,0] #[1,x,y,y']\ntransformation_matrix = np.array([\n   [ 1,0,0,0 ],\n   [ 0,1,0,0 ],\n   [ 0,0,1,0 ],\n   [ 0,2,0,0 ]\n])\nsolution = linear(\n    init_y,\n    expanded_euler,\n    transformation_matrix,\n    steps=100, h=0.01)\n\n\nplt.plot(solution.x, solution.y_0, label='Approximated Solution')\nplt.plot(solution.x, solution.x**2, label='True Solution', linestyle='--')\nplt.xlabel('x') # Label for the x-axis\nplt.ylabel('y') # Label for the y-axis\nplt.grid(True) # Show a grid for better readability\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "content/post/taylor-series/post-2.html",
    "href": "content/post/taylor-series/post-2.html",
    "title": "Making a Python Library to solve differential Equations",
    "section": "",
    "text": "After having the initial idea I wrote up in a previous post, I thought it was a good idea to turn it into a python library so that I can use it as part of my other projects.\nIt also gives me a chance to see numerically how well the new method works compared to the Euler method.\n\nFirst Steps\nSo in the last post I set out the method such that:\n\\[\n\\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} =  S \\cdot \\begin{bmatrix}\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix} + \\epsilon\n\\]\nIn the Euler method, \\(S\\) is:\n\\[\n\\begin{bmatrix}\n1 & h & 0 &  ... & 0\\\\\n0 & 1 & h &  ... & 0\\\\\n0 & 0 & 1 &  ... & 0\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix}\n\\]\nAnd in the new method I proposed, \\(S\\) is now:\n\\[\n\\begin{bmatrix}\n1 & \\frac{h}{1!} & \\frac{h^2}{2!} &  ... & \\frac{h^n}{n!}\\\\\n0 & 1 & \\frac{h}{1!} &  ... & \\frac{h^{n-1}}{(n-1)!}\\\\\n0 & 0 & 1 &  ... & \\frac{h^{n-2}}{(n-2)!}\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix}\n\\]\nConverting these matrices into python is fairly easy.\n\nimport numpy as np\nimport math\n\n\ndef euler(dims, h):\n    # Start with an identity matrix\n    step_matrix = np.identity(dims)\n    # Add in all the h values\n    for i in range(dims - 1):\n        step_matrix[i, i + 1] = h\n    return step_matrix\n\n\ndef expanded_euler(dims, h):\n    step_matrix = np.zeros((dims, dims))\n    for i in range(dims):\n        for j in range(i, dims):\n            # Is 1, and h at j-i =0, 1 respectively\n            step_matrix[i, j] = h ** (j - i) / math.factorial(j - i)\n    return step_matrix\n\n\n\nMaking a step simulation\nNow that we have the stepping matrices, we can use them to iterate from an initial value. All we have to do is generate the stepping matrix for the given problem, and then for each step, we just multiple the previous step by the stepping matrix.\n\ndef IVP(x, y, step_matrix_generator, steps=10, h=0.1):\n    dims = len(y)\n    step_matrix = step_matrix_generator(dims, h)\n    output_dict = {x: y}\n\n    x_n = x\n    y_n = y.copy()\n    i = 0\n    while i &lt; steps:\n        y_n = step_matrix @ y_n\n        x_n += h\n        output_dict[x_n] = y_n\n        i += 1\n\n    return output_dict\n\n\n\nTesting and Comparing the methods\nNow we can run the simulations, let’s see how good they are. Say you throw a ball up in the air and track its vertical position. The path of the ball is described by the equation \\(y'' = -9.8\\). We can know for a fact that the solution to this equation is \\(\\frac{-9.8}{2}x^2+V_0x+P_0\\), where \\(V_0\\) is the initial velocity and \\(P_0\\) is the initial position. So now lets compare the real solutions to the simulations.\n\n# Time starts at 0\nx = 0\n# Start the object moving upwards with a velocity of 10\ny = np.array([0, 10, -9.8])\n\neuler_result = IVP(x, y, euler)\nexpanded_euler_result =IVP(x, y, expanded_euler)\ntrue_result = {x: np.array([\n                    -4.9 * x**2 + 10 * x,\n                    -9.8 * x + 10,\n                    -9.8\n                ]) for x in np.arange(0, 1.1, 0.1)}\n\n\n\n/tmp/ipykernel_3827/1645241578.py:3: DeprecationWarning: The catppuccin-matplotlib package is deprecated, please upgrade to https://github.com/catppuccin/python (pip install catppuccin)\n\n\n  import mplcatppuccin\n\n\n\n\n\n\n\n\n\nSo from here, we’re looking pretty good. The new method is much closer to the true solution than the Euler method in in this scenario. However, when working with numerical methods, it generally isn’t too hard to improve the accuracy of the model, but there will be a trade off in computation time. So lets see how much longer it takes to compute the approximation with the expanded method comparing it to the original.\n\nimport timeit\n\n# Define the step counts to test\nsteps_list = [10, 100, 1000, 10000, 100000]\n\n# Lists to store execution times for each method\neuler_times = []\nexpanded_euler_times = []\n\n# Testing the functions with the different step counts and store the execution times\nfor steps in steps_list:\n    euler_time = timeit.timeit(lambda: IVP(x, y, euler, steps), number=1)\n    expanded_euler_time = timeit.timeit(lambda: IVP(x, y, expanded_euler, steps), number=1)\n\n    euler_times.append(euler_time)\n    expanded_euler_times.append(expanded_euler_time)\n\n\n\n\n\n\n\n\n\n\nLooking at this graph, we can see that we’re not sacrificing compute time for better accuracy, so this seems like a big win, though I haven’t optimised the Euler method that much. But overall, the new method seems to show some promise in approximating differential equations."
  },
  {
    "objectID": "content/post/cross-posts/3cr-interview.html",
    "href": "content/post/cross-posts/3cr-interview.html",
    "title": "Interview on Community Radio",
    "section": "",
    "text": "I had the pleasure of talking to Hélène from 3CR about climate misinformation, my work, and the work we do at the MCCCRH."
  },
  {
    "objectID": "content/post/tag-graph/post.html",
    "href": "content/post/tag-graph/post.html",
    "title": "A new way to look at Categories in Hugo Blogs",
    "section": "",
    "text": "Update: This site is now built around Quarto, but the basis of this idea still holds.\nThis site is built through blogdown, which is a marvellous little R package that integrates Hugo sites into a R workflow. A nice thing about Hugo sites are the large number of themes available to quickly integrate with your site to change the vibe. My theme is mostly just a recolored version of the hello-friend-ng theme, using catppuccin for the colors.\nMy only gripe with the theme is that it uses a default list for all types of content. If you want to look at all the posts, you see a list of posts; for tags, similarly, you get a list of tags."
  },
  {
    "objectID": "content/post/tag-graph/post.html#fetching-all-the-posts",
    "href": "content/post/tag-graph/post.html#fetching-all-the-posts",
    "title": "A new way to look at Categories in Hugo Blogs",
    "section": "Fetching All the Posts",
    "text": "Fetching All the Posts\nThe first thing to do is to fetch all the posts from across the site with their tags.\n// Create an array to hold all posts and their details\nvar postsData = [];\n\n// Iterate through each page to collect its title, permalink, and tags\n{{ range .Site.RegularPages }}\n  // Use 'jsonify' to convert tags to a JSON array, if tags are not present, default to an empty array\n  var tags = {{ if .Params.tags }}{{ .Params.tags }}{{ else }}[]{{ end }};\n  postsData.push({\n    title: {{ .Title }},\n    permalink: {{ .RelPermalink }},\n    tags: tags // This is now a real JavaScript array\n  });\n{{ end }}\n\n// Log the posts information in the console as JSON\nconsole.log(postsData);\n\nvar tagsToPosts = {};\n\n// Loop through each post\npostsData.forEach(function(post) {\n  // Loop through each tag in the current post\n  post.tags.forEach(function(tag) {\n    // If the tag hasn't been added to tagsToPosts, initialize it with an empty array\n    if (!tagsToPosts.hasOwnProperty(tag)) {\n      tagsToPosts[tag] = [];\n    }\n    // Add the current post to the array for this tag\n    tagsToPosts[tag].push({\n      title: post.title,\n      permalink: post.permalink\n    });\n  });\n});"
  },
  {
    "objectID": "content/post/tag-graph/post.html#invert-the-keys-of-the-list",
    "href": "content/post/tag-graph/post.html#invert-the-keys-of-the-list",
    "title": "A new way to look at Categories in Hugo Blogs",
    "section": "Invert the keys of the list",
    "text": "Invert the keys of the list\nNow that there is a list of posts with there tags, we can now invert the list so that we have all the tags with their posts.\nvar tagsToPosts = {};\n\n// Loop through each post\npostsData.forEach(function(post) {\n  // Loop through each tag in the current post\n  post.tags.forEach(function(tag) {\n    // If the tag hasn't been added to tagsToPosts, initialize it with an empty array\n    if (!tagsToPosts.hasOwnProperty(tag)) {\n      tagsToPosts[tag] = [];\n    }\n    // Add the current post to the array for this tag\n    tagsToPosts[tag].push({\n      title: post.title,\n      permalink: post.permalink\n    });\n  });\n});\n// Log the new tags to posts dictionary\nconsole.log(tagsToPosts);"
  },
  {
    "objectID": "content/post/tag-graph/post.html#make-a-matrix-of-connections",
    "href": "content/post/tag-graph/post.html#make-a-matrix-of-connections",
    "title": "A new way to look at Categories in Hugo Blogs",
    "section": "Make a matrix of connections",
    "text": "Make a matrix of connections\nWith the list of tags, we can now make a matrix showing all of the connections between the tags. This is really inefficient but it works well enough that I don’t think it requires too much optimisation.\n// Get all unique tags\nvar uniqueTags = Object.keys(tagsToPosts);\n\n// Initialize the matrix with zeros\nvar tagMatrix = uniqueTags.map(() =&gt; uniqueTags.map(() =&gt; 0));\n\n// Function to check the intersection of posts for two tags\nfunction getSharedPostsCount(tagA, tagB, tagsToPosts) {\n  var postsA = tagsToPosts[tagA];\n  var postsB = tagsToPosts[tagB];\n  var shared = postsA.filter(postA =&gt; postsB.some(postB =&gt; postA.permalink === postB.permalink));\n  return shared.length;\n}\n\n// Populate the matrix with shared post counts\nfor (let i = 0; i &lt; uniqueTags.length; i++) {\n  for (let j = i; j &lt; uniqueTags.length; j++) {\n    // We only need to calculate the upper triangular matrix due to symmetry\n    var sharedCount = getSharedPostsCount(uniqueTags[i], uniqueTags[j], tagsToPosts);\n    tagMatrix[i][j] = sharedCount;\n    tagMatrix[j][i] = sharedCount; // The matrix is symmetric so we mirror the count\n  }\n}\n\n// Log the matrix\nconsole.log(tagMatrix);"
  },
  {
    "objectID": "content/post/tag-graph/post.html#build-the-graph",
    "href": "content/post/tag-graph/post.html#build-the-graph",
    "title": "A new way to look at Categories in Hugo Blogs",
    "section": "Build the graph",
    "text": "Build the graph\nBuilding the graph was honestly one of the easiest parts of this project. The d3js library has an object called a force directed graph, has most of the features I would want baked in.\nThe only interesting thing I’ve done here is add in on-click links to the nodes and the edges that allow you to click through to see the relevant posts.\nvar width = window.innerWidth;\nvar height = window.innerHeight;\n\n// Setup the window\nvar svg = d3.select(\"body\")\n  .append(\"svg\")\n  .attr(\"width\", width)\n  .attr(\"height\", height)\n  .style(\"display\", \"block\")\n  .style(\"margin\", \"auto\");\n\nvar nodeRadius = 60;\nvar collisionRadius = nodeRadius * 2;\n\n// Setup the simulation\nvar simulation = d3.forceSimulation(nodes)\n  .force(\"link\", d3.forceLink(links)\n    .id(d =&gt; d.id)\n    .distance(200)\n    .strength(d =&gt; 0.1 * d.value))\n  .force(\"charge\", d3.forceManyBody()\n    .strength(d =&gt; -500 * (d.value + 1)))\n  .force(\"center\", d3.forceCenter(width / 2, height / 2))\n  .force(\"collision\", d3.forceCollide(collisionRadius))\n  .alphaDecay(0.01);\n\n// System for dragging\nvar dragHandler = d3.drag()\n  .on(\"start\", function(d) {\n    if (!d3.event.active) simulation.alphaTarget(0.3).restart();\n    d.fx = d.x;\n    d.fy = d.y;\n  })\n  .on(\"drag\", function(d) {\n    d.fx = d3.event.x;\n    d.fy = d3.event.y;\n  })\n  .on(\"end\", function(d) {\n    if (!d3.event.active) simulation.alphaTarget(0);\n    d.fx = null;\n    d.fy = null;\n  });\n\n// Draw the line between the nodes\nvar link = svg.append(\"g\")\n  .attr(\"class\", \"links\")\n  .selectAll(\"line\")\n  .data(links)\n  .enter().append(\"line\")\n  .attr(\"stroke-width\", d =&gt; Math.sqrt(d.value) + 10)\n  .attr(\"stroke\", \"#cad3f5\")\n  .on(\"click\", function(d) {\n    console.log(`Link clicked between ${d.source.id} and ${d.target.id}`);\n    let tagName1 = d.source.id.replace(/\\s+/g, '-').toLowerCase();\n    let tagName2 = d.target.id.replace(/\\s+/g, '-').toLowerCase();\n    window.location.href = `/tags/${tagName1}?tag=${tagName2}`;\n  });\n\nvar colors = [\n  '#f4dbd6',\n  '#b7bdf8',\n  '#ed8796',\n  '#f5a97f',\n  '#eed49f',\n  '#a6da95',\n  '#8bd5ca',\n  '#8aadf4',\n  '#c6a0f6',\n  '#f0c6c6'\n];\n\nfunction getRandomColor() {\n  return colors[Math.floor(Math.random() * colors.length)];\n}\n\n\n\n// Make the nodes\nvar node = svg.append(\"g\")\n  .attr(\"class\", \"nodes\")\n  .selectAll(\"circle\")\n  .data(nodes)\n  .enter().append(\"circle\")\n  .on(\"click\", function(d) {\n    let tagName = d.id.replace(/\\s+/g, '-').toLowerCase();\n    window.location.href = `/tags/${tagName}`;\n  })\n  .attr(\"r\", nodeRadius)\n  .attr(\"fill\", function(d) { return getRandomColor(); });\n\nnode.append(\"title\")\n  .text(d =&gt; d.id);\n\nsimulation\n  .nodes(nodes)\n  .on(\"tick\", ticked);\n\nsimulation.force(\"link\")\n  .links(links);\n\ndragHandler(node);\n\n// Wirite out the labels\nvar labels = svg.append(\"g\")\n  .attr(\"class\", \"labels\")\n  .selectAll(\"text\")\n  .data(nodes)\n  .enter().append(\"text\")\n  .text(d =&gt; d.id)\n  .style(\"fill\", \"24273a\")\n  .style(\"text-anchor\", \"middle\")\n  .style(\"dominant-baseline\", \"central\")\n  .on(\"click\", function(d) {\n    let tagName = d.id.replace(/\\s+/g, '-').toLowerCase();\n    window.location.href = `/tags/${tagName}`;\n  });\n\n// Update function\nfunction ticked() {\n  link\n    .attr(\"x1\", d =&gt; d.source.x)\n    .attr(\"y1\", d =&gt; d.source.y)\n    .attr(\"x2\", d =&gt; d.target.x)\n    .attr(\"y2\", d =&gt; d.target.y);\n\n  node\n    .attr(\"cx\", d =&gt; d.x)\n    .attr(\"cy\", d =&gt; d.y);\n\n  labels\n    .attr(\"x\", d =&gt; d.x)\n    .attr(\"y\", d =&gt; d.y);\n}"
  },
  {
    "objectID": "content/post/data-mining/post-1.html",
    "href": "content/post/data-mining/post-1.html",
    "title": "The Urgent Need for Data Access in the Age of AI",
    "section": "",
    "text": "Climate misinformation is one of the great challenges of our age, with its propagators ranging from distant strangers on Facebook or Twitter to the President of the United States. It misleads the public, undermines scientific consensus, and delays necessary actions to address climate change, exacerbating its impacts. However, there appears little we can do to combat it. With the current legal landscape, the tools and systems we need to fight the spread of misinformation aren’t at our disposal; conversely, the perpetrators of misinformation are able to expand their efforts with AI technologies and social media networks, leaving the fact-checkers of the world on an uneven playing field.\nIn response to misinformation, fact-checkers have attempted to inform readers how they have been misled, publishing their work countering the misleading statements. This work continues today, with ABC RMIT Fact Check and the Australian Associated Press being notable examples. But increasingly, these organisations are struggling to make an impact due to lack of trust, especially amongst people who identify as right-leaning – the group that believes the most in climate misinformation. Fact-checkers also have to deal with a media landscape where anyone can say something and publish it online, greatly expanding the scale of what they need to cover.\nOne of the most promising new methods of protecting the general public against climate change misinformation has been the concept of pre-bunking, where readers see the misinformation being debunked before they ever see the misinformation. However, to pre-bunk misinformation you need to quickly know what misleading claims exist and where they are, and in our current legal system, that job is challenging.\nWith AI slowly creeping into our everyday lives, you might think that it could be part of the solution to this problem. But you asked your favourite large language model, such as ChatGPT, to grab the top stories from ABC News Australia, you might get a response similar to this:\n\n“It seems I’m unable to access ABC Australia’s website directly at the moment. However, you can check the latest top stories by visiting their ABC News page, which features the most recent and important updates from Australia and the world.”\n\nNow, this response isn’t because the model is incapable of accessing the page, but because the ABC doesn’t allow it. The ABC, like every other news organisation, explicitly denies scraping of their website for data in their terms and conditions. This blanket ban isn’t completely unfounded, since AI companies have been known to use copyrighted material in their training, and news organisations have every right to protect their copyrighted material. However, this kind of heavy-handed blocking of scraping means that fact-checkers aren’t able to legally collect information about what is being published in the Australian media landscape.\nBut let’s say we’re not so interested in the spread of misinformation by our news organisations, but rather only through social media. Here again, we face the same issue. If you start scraping data off Facebook, Instagram, Reddit or X without the express permission of the companies in question, it won’t be long until the lawyers start knocking. Fortunately, these companies have been generally cooperative, offering researchers, businesses, and individuals access to data through authorised channels. However, these methods are quickly becoming more restrictive, with both Reddit and Instagram recently deciding to remove most of their accessible routes in the name of protecting their data and income. If this trend continues, fact-checkers will quickly lose the ability to see what misinformation exists on social media.\nUnfortunately, fact-checkers rely on this data, and therefore some choose to collect it from these sources, even if it constitutes a legal grey area. They hope that so long as they don’t do anything to provoke these large media or tech companies, they should be able to fly under the radar and continue operating. But for many, the risk of litigation is too great. Under current laws, media companies decide who can access their data. This means these platforms can spread misinformation without any fact-checkers holding them accountable. If we truly want to stop climate misinformation, we need to give fact-checkers the tools to combat it: the data."
  },
  {
    "objectID": "content/post/hottest-100/post-2.html",
    "href": "content/post/hottest-100/post-2.html",
    "title": "A Follow Up on my Hottest 100 Predictions",
    "section": "",
    "text": "In my last post I ended by planting my flag and making my predictions for the Hottest 100 for 2023. And on first glance, I’m pretty happy with myself, picking not only the top song, but a good chunk of the top 20. So today I just want to do a quick follow up on how I did.\n\nTop 20\n\n\n\n\n\nSong\nArtist\nPredicted\n\n\n\n\npaint the town red\ndoja cat\n1\n\n\nthe worst person alive\ng flip\n55\n\n\nsaving up\ndom dolla\nNA\n\n\nrhyme dust\nmk\n72\n\n\nprada\ncass\n90\n\n\nadore u\nfred again\n40\n\n\nwhat was i made for?\nbillie eilish\n3\n\n\nrush\ntroye sivan\n10\n\n\nlovin on me\njack harlow\n17\n\n\nchemical\npost malone\n14\n\n\nvampire\nolivia rodrigo\n2\n\n\nrumble\nskrillex\nNA\n\n\nkill bill\nsza\n9\n\n\natmosphere\nfisher\nNA\n\n\nnanana\npeggy gou\n5\n\n\nsprinter\ndave\n4\n\n\nback on 74\njungle\nNA\n\n\neat your man\ndom dolla\nNA\n\n\ntherapy\nbudjerah\nNA\n\n\nsorry instead\nspacey jane\nNA\n\n\n\n\n\nThe top 10 seems okay, but i did miss a lot of the top 20 completely. I seemed to be undervaluing Australian artists such as Dom Dolla, Spacey Jane, and G Flip, as well as EDM as a genre, which made up a much greater portion of the top 20 than I predicted.\n\n\nSnubbed Songs\n\n\n\n\n\nSong\nArtist\nPredicted\n\n\n\n\nlove type\npoolclvb\n6\n\n\nsuper ego\nbabe rainbow\n8\n\n\nadored\nroyel otis\n15\n\n\nlost without you\nsan cisco\n21\n\n\nsuper-vision\ndice\n22\n\n\nblak britney\nmiss kaninna\n24\n\n\nruthless\nhooligan hefs\n26\n\n\nattention\ndoja cat\n27\n\n\njames dean\ntash sultana\n28\n\n\ndo it again\nbenee\n33\n\n\ni wish you roses\nkali uchis\n35\n\n\ndash of speed\nrum jungle\n36\n\n\ntoo much\nthe kid laroi\n37\n\n\neyes ahead\ndice\n38\n\n\npets and drugs\nthe rubens\n42\n\n\ninto your room\nholly humberstone\n44\n\n\nup\nlee\n46\n\n\npegasus\narlo parks\n47\n\n\nexploding\nangie mcmahon\n48\n\n\ncalling\nmetro boomin\n50\n\n\nmust be nice\nruel\n51\n\n\nbetter love\neliza rose\n52\n\n\nyour funeral\nmaya\n53\n\n\ndon’t let me down\ngus dapperton\n54\n\n\nconceited\nlola young\n56\n\n\nmessed up\nholy holy\n57\n\n\nlost in the rush\ntelenova\n58\n\n\ncandle flame\njungle\n59\n\n\nprescription\nremi wolf\n63\n\n\nchange\nlaurel\n64\n\n\nangel\npinkpantheress\n65\n\n\nsinner\nthe last dinner party\n66\n\n\nmidwest\nvacations\n67\n\n\npretty girl\nice spice\n68\n\n\ni wanna dance\nroyel otis\n71\n\n\nmake up your mind\ncordae\n74\n\n\nviper room\nthornhill\n75\n\n\nweightless\narlo parks\n76\n\n\ncan’t play myself\nskepta\n78\n\n\nblack mascara.\nraye\n79\n\n\nfeel alive\ncat\n80\n\n\ntied up!\ngenesis owusu\n81\n\n\nmrs. hollywood\ngojo\n84\n\n\nam i dreaming\nmetro boomin\n85\n\n\ncomma’s\nonefour\n86\n\n\nlola\nmaya\n87\n\n\nit’s cool to be in love\ngreta stanley\n88\n\n\nsince i have a lover\n6lack\n89\n\n\nlil boo thang\npaul russell\n91\n\n\ngoing kokomo\nroyel otis\n93\n\n\nall my life\nlil durk\n94\n\n\nbrain freeze\nnortheast party house\n95\n\n\nhealing\nmolly millington\n96\n\n\nsticky\nkito\n97\n\n\nrakata\nthe jungle giants\n98\n\n\n\n\n\nSo there were 55 songs in my predictions that didn’t make it into the countdown, including 3 of my top 20. Funily enough though, a lot of my predictions seemed to line up with peoples opions online with Love Type, Super Ego and Adored all being mentioned as snubs from the hottest 100.\n\n\nSurprise Songs\n\n\n\n\n\nSong\nArtist\nrank\n\n\n\n\nsaving up\ndom dolla\n3\n\n\nrumble\nskrillex\n12\n\n\natmosphere\nfisher\n14\n\n\nback on 74\njungle\n17\n\n\neat your man\ndom dolla\n18\n\n\ntherapy\nbudjerah\n19\n\n\nsorry instead\nspacey jane\n20\n\n\nbe your man\ng flip\n22\n\n\ntake it off\nfisher\n23\n\n\nrich baby daddy\ndrake\n25\n\n\nrough\ng flip\n26\n\n\ndance the night\ndua lipa\n28\n\n\nsay yes to heaven\nlana del rey\n29\n\n\nnot strong enough\nboygenius\n30\n\n\nget him back!\nolivia rodrigo\n34\n\n\nbaby again..\nfred again\n35\n\n\nboy’s a liar pt. 2\npinkpantheress\n36\n\n\nlaced up\nhilltop hoods\n37\n\n\nscary movies\nthe rions\n40\n\n\npedestal\nlime cordiale\n42\n\n\npopular\nthe weeknd\n43\n\n\nsweetheart\nold mervs\n47\n\n\npadam padam\nkylie minogue\n48\n\n\naustralia\ng flip\n50\n\n\nten\nfred again\n51\n\n\ni used to be fun\nteen jesus\n52\n\n\nlookin’ out\nking stingray\n53\n\n\nmore than you know\nblink182\n54\n\n\nall-american bitch\nolivia rodrigo\n56\n\n\ndarkside\nbring me the horizon\n57\n\n\nlost\nbring me the horizon\n58\n\n\na&w\nlana del rey\n60\n\n\nfall at your feet\npeking duk\n64\n\n\nreal life\ng flip\n65\n\n\nleaving the light\ngenesis owusu\n69\n\n\nsnooze\nsza\n70\n\n\nspeedracer\nteenage dads\n74\n\n\nnobody gets me\nsza\n75\n\n\nsofa king\nroyel otis\n76\n\n\ni don’t wanna be …\nruel\n77\n\n\nbleed\nthe kid laroi\n78\n\n\nvideo killed the …\nteenage dads\n79\n\n\n7 days\ng flip\n80\n\n\nlike a girl does\npeach prc\n81\n\n\nexes\ntate mcrae\n82\n\n\nthe summoning\nsleep token\n83\n\n\nmidnight driving\nteenage dads\n88\n\n\nnightmare\npolaris\n90\n\n\ndid you know that…\nlana del rey\n91\n\n\nstrawberry daydream\npacific avenue\n92\n\n\nno bad days\nthe terrys\n93\n\n\nwelcome to the dcc\nnothing but thieves\n95\n\n\nstay blessed\ngenesis owusu\n97\n\n\ncool about it\nboygenius\n98\n\n\ni miss you\nslowly slowly\n99\n\n\n\n\n\nWith 55 snubs, we are going to have 55 surprise songs. There doesn’t seem to be a massive trend here. G flip only made it into my countdown twice, so 5 of their songs are in this list. Its also interesting seeing which conventionally popular songs are part of this list. Boy’s a liar pt. 2 by Pinkpantheress and All American-Bitch which peaked at 2 and 10 on the aria charts were left out in my predictions, even though it predicted similar chart toppers in the top 10.\n\n\n\n\n\n\n\n\n\nLooking at these plots, it seems that the chart data for these songs didn’t correctly join with the play data from triple J. After cleaning the names, I was hoping that there wouldn’t be too much of a discrepancy. However, in the ARIA charts, “all-american bitch” is listed as “all-american b**ch,” and “boys a liar pt. 2” is listed as “boys a liar”. This kind of discrepancy is probably present throughout my dataset and may have led to some major inaccuracies. However, it is also just part of life when dealing with text data.\n\n\nDid I do better than Warm Tuna?\nPart of my mission when setting out to make these predictions was to outperform 100 Warm Tunas, who utilize a compilation of social media posts to formulate their predictions.\n\n\n\n\n\nSong\nArtist\nrank\nwarm_tuna\nmy_rankning\n\n\n\n\npaint the town red\ndoja cat\n1\n9\n1\n\n\nthe worst person alive\ng flip\n2\n11\n55\n\n\nsaving up\ndom dolla\n3\n18\nNA\n\n\nrhyme dust\nmk\n4\n5\n72\n\n\nprada\ncass\n5\n19\n90\n\n\nadore u\nfred again\n6\n6\n40\n\n\nwhat was i made for?\nbillie eilish\n7\n4\n3\n\n\nrush\ntroye sivan\n8\n1\n10\n\n\nlovin on me\njack harlow\n9\n49\n17\n\n\nchemical\npost malone\n10\n28\n14\n\n\nvampire\nolivia rodrigo\n11\n14\n2\n\n\nrumble\nskrillex\n12\n3\nNA\n\n\nkill bill\nsza\n13\n20\n9\n\n\natmosphere\nfisher\n14\n23\nNA\n\n\nnanana\npeggy gou\n15\n15\n5\n\n\nsprinter\ndave\n16\n22\n4\n\n\nback on 74\njungle\n17\n13\nNA\n\n\neat your man\ndom dolla\n18\n59\nNA\n\n\ntherapy\nbudjerah\n19\n8\nNA\n\n\nsorry instead\nspacey jane\n20\n21\nNA\n\n\n\n\n\nStraight away, I can see that warm tuna did better than me, but by how much?\nSo, I made up a quick statistic to see how far off our predictions were. This is the sum of the magnitudes of the differences between the predicted score and the actual score. If a song didn’t make the top 100, it’s given the equivalent rank of 101. I then divide this by 100 to get the average deviation for each prediction.\n\n\nmy score: 36.28 \n\n\nwarm tuna's score: 26.5 \n\n\nSo from these statistics, we can see that my predictions were, on average, about 10 places more off than warm tuna’s.\n\n\nNext Year?\nI reckon this method still has promise, but I need to sort out the name joining issue to ensure that my method is working at its maximum potential. I also want to include genre and artist country into it since it seemed to be an important factor in the final rank that I didn’t account for."
  },
  {
    "objectID": "content/post/dev-containers/post-1.html",
    "href": "content/post/dev-containers/post-1.html",
    "title": "DIY Dev-Containers",
    "section": "",
    "text": "Like most developers, I spend an inordinate amount of time dealing with my local installations and dependencies. When working on multiple projects, it is not uncommon to encounter conflicting versions of dependencies, and while virtual environments and package managers like Node Package Manager help to mitigate this issue, they often fall short.\n\nWhy we use Dev-Containers\nA common solution to these issues is the use of ‘dev-containers’, which have mostly been popularized by VS Code as a way to have your dependencies exist exclusively inside a Docker container, and then attach an editor to it to make your changes. Sounds great, but unfortunately for me, I have years of using Vim keybindings built into my muscle memory, so there’s little chance of me changing my editor. So instead, I thought, why not just rebuild the dev containers for Vim?\n\n\nWhat I want\nSo let’s quickly scope out this project. In my development containers, I want:\n\nIsolated environments\nVim with my configuration built-in\nIntegration with common CLI tools\nThe ability to use Docker from inside the container\nSecrets management (not having to re-authenticate all my tools every time I open up a container)\nTransportability between various Unix machines\n\n\n\nThe Beginnings\nSo after taking a quick look around my system, I have come up with this initial Dockerfile for my development container:\nFROM ubuntu as setter_upper\n\nARG DEBIAN_FRONTEND=noninteractive\nENV TZ=Australia/Melbourne\n# Enviroment Installs\nRUN apt-get update && apt-get install -y \\\n   curl git python3 python3-pip apt-transport-https \\\n   ca-certificates software-properties-common  libpq-dev \\\n   build-essential autoconf automake libtool\n\n#Install Docker\nRUN curl -fsSL https://get.docker.com -o install-docker.sh\nRUN sh install-docker.sh\n\n\n# Install GH CLI\nRUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&& chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&& echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | tee /etc/apt/sources.list.d/github-cli.list &gt; /dev/null \\\n&& apt update \\\n&& apt install gh -y\n\n# git\n#RUN gh auth setup-git\nrun git config --global user.name \"Fonzzy1\"\nrun git config --global user.email \"alfiechadwick@hotmail.com\"\n\n# Set the base work dir\nWORKDIR /src\n\n# Set the mount point as the safe dir\nRUN git config --global --add safe.directory /src\n\n# Vim Setup\nFROM setter_upper as vim\n\n# Enviroment Installs\nRUN apt-get update && apt-get install -y software-properties-common\nRUN add-apt-repository ppa:jonathonf/vim\nRUN apt-get update\n\n# Install the rest of the dependencies\nRUN apt-get install -y \\\n    tig \\\n    fzf \\\n    pkg-config \\\n    texlive \\\n    r-base \\\n    pandoc \\\n    texlive-latex-extra \\\n    libcurl4-openssl-dev \\\n    libssl-dev \\\n    libxml2-dev \\\n    libfontconfig1-dev \\\n    libharfbuzz-dev \\\n    libfribidi-dev \\\n    libfreetype6-dev \\\n    libpng-dev \\\n    libtiff5-dev \\\n    libjpeg-dev \\\n    r-cran-tidyverse \\\n    vim-gtk3\n\n#Install Ctags\nRUN curl -L https://github.com/thombashi/universal-ctags-installer/raw/master/universal_ctags_installer.sh | bash\n\n# Install node\nRUN set -uex\nRUN mkdir -p /etc/apt/keyrings\nRUN curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\nRUN echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main\" |  tee /etc/apt/sources.list.d/nodesource.list\nRUN apt-get update && apt-get install nodejs -y;\n\n\n# Install the python packages\nRUN pip install black pipreqs pgcli awscli socli\n\n# Install npm packages\nRUN npm install --save-dev --global prettier\n\n# Download and Install Vim-Plug\nRUN curl -fLo /root/.vim/autoload/plug.vim --create-dirs \\\n    https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n\n# Install ACT extention\nRUN mkdir -p /root/.local/share/gh/extensions/gh-act\nRUN curl -L -o /root/.local/share/gh/extensions/gh-act/gh-act \\\n    \"https://github.com/nektos/gh-act/releases/download/v0.2.57/linux-amd64\"\nRUN chmod +x /root/.local/share/gh/extensions/gh-act/gh-act\n\n\n# Install R packages, tidyvverse is installed with apt\nRUN R -e  \"install.packages('rmarkdown',  Ncpus = 6)\"\nRUN R -e  \"install.packages('reticulate',  Ncpus = 6)\"\nRUN R -e  \"install.packages('blogdown',  Ncpus = 6)\"\nRUN R -e  \"blogdown::install_hugo()\"\nRUN R -e  \"install.packages('readxl',  Ncpus = 6)\"\nRUN R -e  \"install.packages('knitr',  Ncpus = 6)\"\nRUN R -e  \"install.packages('tinytex',  Ncpus = 6)\"\nRUN R -e  \"install.packages('languageserver',  Ncpus = 6)\"\n\n# Bring in the vim config\nCOPY vim /root/.vim\n#Copy in the dotfiles\nCOPY dotfiles /root\n\n# Install Vim Plugins\nRUN vim +PlugInstall +qall\n\n# Install COC plugins\nRUN mkdir -p /root/.config/coc/extensions && \\\n    echo '{\"dependencies\":{}}' &gt; /root/.config/coc/extensions/package.json && \\\n    grep 'let g:coc_global_extensions' /root/.vim/config/coc.vim | \\\n    sed \"s/.*\\[//; s/\\].*//; s/'//g; s/, /\\n/g\" | \\\n    while read -r extension; do \\\n        echo \"Installing coc extension: $extension\" && \\\n        cd /root/.config/coc/extensions && \\\n        npm install \"$extension\" --install-strategy=shallow --save; \\\n    done\n\nCMD vim\nI won’t bother explaining most of it since it’s really just a heap of install statements, but here are some of the interesting parts:\n\nI needed to add the WORKDIR to the list of safe directories for git since if I mount the file, the ownership will be wrong.\nI needed to manually install the gh act extension as you can’t do it normally without authenticating with a gh token, something I don’t want to do in a public container.\nCoc Extensions needed to be manually installed to prevent them from installing every time I started the container. Just calling Vim +CocInstall didn’t work because it’s an async process.\n\nSo at this point, I have the first three of my requirements done. Because I’m using Docker, I have an isolated environment every time I boot up the container. By copying over my Vim config files, I have my Vim config baked in, and with some of the commands in the Dockerfile, I am able to have it set up. Finally, by installing a heap of CLI tools, I am able to do most of my work from inside the Vim terminal.\n\n\nDocker In Docker\nThe next thing to tick off the list is being able to run Docker commands from within the container. Although I have installed Docker, running any Docker command inside the container will say the daemon isn’t running.\nI could put in a lot of work to give the container the ability to create its own containers, but that would be a real pain. Instead, I can simply mount the Docker daemon onto the container, so that running Docker commands inside the container will invoke the system Docker.\nTo accomplish this, I can execute the container using the following command:\ndocker run -it -v /var/run/docker.sock:/var/run/docker.sock fonzzy1/vim\n\n\nSecrets Management\nThe next thing to implement is secrets management. I currently have all of these stored in config files in my home directory, which isn’t best practice in a Docker container that I want to make public. Instead, I can put all my secrets in a .env file and reference them in the Docker container. This can be done using the –env-file flag when running my Docker container.\n\n\nPortability\nThe final goal on my list is to make the container portable between my multiple machines. This is achieved through the use of Docker Hub, which will allow me to download the image from Docker Hub. The only other thing I need is to ensure that Docker is set up on the other machine. For this, I have written a quick script to handle the setup process.\n#!/bin/bash\nset -e\n\n# Dot Progress Indicator\nprogress() {\n    local pid=$2 # PID of the process we're waiting for\n    local text=$1\n    local delay=2 # 2-second delay between dots\n    local dot=\".\"\n\n    printf \"%s:\" \"$text\"\n    while [ \"$(ps a | awk '{print $1}' | grep -w $pid)\" ]; do\n        printf \"%s\" \"$dot\"\n        sleep $delay\n    done\n    printf \" Done!\\n\"\n}\n\nprogress \"Updating package list\" $(sudo apt-get update &gt; /dev/null 2&gt;&1 & echo $!)\n\nprogress \"Installing Useful Packages\" $(sudo apt-get install -y curl &gt; /dev/null 2&gt;&1 & echo $!)\n\nprogress \"Fetching Docker Install Script\" $(curl -fsSL https://get.docker.com -o install-docker.sh &gt; /dev/null 2&gt;&1 & echo $!)\n\nprogress \"Installing Docker\" $(sudo sh install-docker.sh &gt; /dev/null 2&gt;&1 & echo $!)\n\nprogress \"Adding the current user to the Docker group\" $(sudo usermod -aG docker $USER &gt; /dev/null 2&gt;&1 & echo $!)\n\nprogress \"Pulling Image\" docker pull fonzzy1/vim\n\necho \"Setup complete!\"\n\n\nWrapping it up\nMy so now I have my dev containers running, my only gripe is the stupidly long docker commands that I need to type out to get it running, such as:\ncurrent_dir=\"$(pwd)\"\ndir_name=\"$(basename \"$current_dir\")\"\n\ndocker run -it \\\n  --env-file ~/.env \\\n  --net=host \\\n  --rm \\\n  -v \"$current_dir:/$(dir_name)\" \\\n  -w \"/$dir_name\" \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  fonzzy1/vim \\\n  /bin/bash -c \"gh auth setup-git; git config --global --add safe.directory /$dir_name; vim\"\nSo I decided to make this into a little Python script that allows me to quickly run these commands. I also added an integration with gh that lets me clone repos in order to edit them on the fly.\n\n#!/bin/python3\nimport subprocess\nimport argparse\nimport os\n\n\ndef run_local(args):\n    \"\"\"\n    Runs a command in a Docker container with the current directory mounted.\n\n    Args:\n        args (argparse.Namespace): The command-line arguments.\n\n    Returns:\n        None\n    \"\"\"\n    current_dir = subprocess.run([\"pwd\"], capture_output=True, text=True).stdout.strip()\n    dir_name = current_dir.split(\"/\")[-1]  # Get the name of the current directory\n\n    subprocess.run(\n        [\n            \"docker\",\n            \"run\",\n            \"-it\",\n            \"--env-file\",\n            os.path.expanduser(\"~/.env\"),\n            \"--net=host\",\n            \"--rm\",\n            \"-v\",\n            f\"{current_dir}:/{dir_name}\",  # Mount to a directory with the same name\n            \"-w\",\n            f\"/{dir_name}\",  # Set the working directory\n            \"-v\",\n            \"/var/run/docker.sock:/var/run/docker.sock\",\n            \"fonzzy1/vim\",\n            \"/bin/bash\",\n            \"-c\",\n            f\"gh auth setup-git; git config --global --add safe.directory /{dir_name}; vim\",\n        ]\n    )\n\n\ndef run_gh(args):\n    \"\"\"\n    Runs a command for cloning a GitHub repository in a Docker container.\n\n    Args:\n        args (argparse.Namespace): The command-line arguments.\n\n    Returns:\n        None\n    \"\"\"\n    name = args.repo.replace(\"/\", \"-\")\n    repo = args.repo.split(\"/\")[-1] if \"/\" in args.repo else args.repo\n    command = f\"gh auth setup-git; gh repo clone {args.repo} /{repo}; \"\n\n    # Additional git command based on input parameters\n    if args.branch:\n        command += f\"git switch {args.branch}; \"\n    elif args.pullrequest:\n        command += f\"gh pr checkout {args.pullrequest}; \"\n    elif args.checkout:\n        command += f\"git checkout -b {args.checkout}; git push --set-upstream origin {args.checkout}; \"\n\n    # Update submodules if any\n    command += \"git submodule update --init; vim; \"\n\n    # Check for unpushed or uncommitted changes before exiting Vim\n    check_changes_command = ' \\\n        CHANGES=$(git status --porcelain); \\\n        UPSTREAM_CHANGES=$(git cherry -v); \\\n        if [ -n \"$CHANGES\" ] || [ -n \"$UPSTREAM_CHANGES\" ]; then \\\n            vim -c \\':G | only\\'; \\\n        fi'\n\n    # Final combined command\n    final_command = command + check_changes_command\n\n    subprocess.run(\n        [\n            \"docker\",\n            \"run\",\n            \"-it\",\n            \"--env-file\",\n            os.path.expanduser(\"~/.env\"),\n            \"--name\",\n            name,\n            \"--net=host\",\n            \"--rm\",\n            \"-w\",\n            f\"/{repo}\",\n            \"-v\",\n            \"/var/run/docker.sock:/var/run/docker.sock\",\n            \"fonzzy1/vim\",\n            \"/bin/bash\",\n            \"-c\",\n            final_command,\n        ]\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(title=\"commands\", dest=\"command\")\n\n    local_parser = subparsers.add_parser(\n        \"local\", help=\"Run command for a container with local directory\"\n    )\n    local_parser.set_defaults(func=run_local)\n\n    gh_parser = subparsers.add_parser(\"gh\", help=\"Run command for cloning a repo\")\n    gh_parser.add_argument(\"repo\", help=\"Specify the repository for cloning\")\n    gh_parser.set_defaults(func=run_gh)\n    gh_parser.add_argument(\"-b\", \"--branch\", help=\"The branch to checkout\")\n    gh_parser.add_argument(\n        \"-p\", \"--pullrequest\", help=\"The pull request number to checkout\"\n    )\n    gh_parser.add_argument(\"-c\", \"--checkout\", help=\"Checkout a new branch from main\")\n\n    args = parser.parse_args()\n    args.func(args)"
  },
  {
    "objectID": "pages/reviews/reviews.html",
    "href": "pages/reviews/reviews.html",
    "title": "Reviews",
    "section": "",
    "text": "Some Sexy Songs 4 U\n\n          \n          PartyNextDoor and Drake\n\n          \n          \n            Date: May 22, 2025\n            While: Driving to Warburton for Easter Break\n            On: A 2006 Ford Fiesta\n          \n\n          \n          Like any Zillenial, I have fond memories of Drake. I remember \"Hotline Bling\" and \"One Dance\" being the soundtracks to my high school parties. To me, Drake was always associated with dancefloor bangers. Unfortunately, \"Some Sexy Songs 4 U\" doesn't fit that mold. The first half of the album was unmemorable, with the first 10 songs blending into some forgettable RnB, emphasized by Drake's signature \"It's hard when every woman wants me\" autotuned singing. The only reprise was the stint of \"Meet Your Padre,\" \"Nokia,\" and \"Die Trying,\" all of which made for some memorable moments and the only chance to turn up the music in the 75-minute album.\n\n        \n\n        \n        \n          \n            ★\n          \n            ★\n          \n            ★\n          \n            ★\n          \n            ★\n          \n        \n\n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "pages/posts/postList.html",
    "href": "pages/posts/postList.html",
    "title": "Posts",
    "section": "",
    "text": "2025\n\n\n\nApril\n\n\n\nFixing My Relationship with Spotify  15th \n\n\n\nMarch\n\n\n\nQuick Tip for Displaying Imported Definitions in Quarto  26th \n\n\nInterview on Community Radio  10th \n\n\n\n\n2024\n\n\n\nOctober\n\n\n\nThe Urgent Need for Data Access in the Age of AI  14th \n\n\n\nMarch\n\n\n\nOptimising the settings for the ODE solver  15th \n\n\n\nFebruary\n\n\n\nMaximising the Number of Friendship Bracelets for the Taylor Swift Concert  7th \n\n\nA Follow Up on my Hottest 100 Predictions  3rd \n\n\n\nJanuary\n\n\n\nPredicting the 2023 Hottest 100  26th \n\n\nMaking my ODE solver solve ODEs  12th \n\n\nDIY Dev-Containers  7th \n\n\n\n\n2023\n\n\n\nDecember\n\n\n\nMaking a Python Library to solve differential Equations  29th \n\n\nA new way to look at Categories in Hugo Blogs  28th \n\n\nUsing Taylor Series to Improve the Euler Method  18th \n\n\n\nOctober\n\n\n\nModeling Drug Use in Communities  22nd \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alfie Chadwick",
    "section": "",
    "text": "Hiya, I’m Alfie, a PhD student from Melbourne, Australia. My research focuses on automated methods for detecting climate change misinformation, but I also have an interest in a range of fields where technology has the ability to solve problems in the humanities. This blog is just a place to put my projects into one place and practice writing about things that seem cool to me.\n\n\nLatest Posts:\n\n\n\n\n\n\n\n\n\n Fixing My Relationship with Spotify   2025-04-14 \n\n\n Quick Tip for Displaying Imported Definitions in Quarto   2025-03-25 \n\n\n Interview on Community Radio   2025-03-09 \n\n\n\nNo matching items"
  },
  {
    "objectID": "content/post/dev-containers/post-2.html",
    "href": "content/post/dev-containers/post-2.html",
    "title": "Quick Tip for Displaying Imported Definitions in Quarto",
    "section": "",
    "text": "This is a very niche issue that I run into surprisingly often. When I am prototyping something in Python, I end up with my definitions spread across various files that I import into a master file. However, once I go to write it up, I end up needing to copy and paste the content into my Quarto file so I can show the definition. I hate doing this because it means that I have something defined twice, and the definitions might go out of sync, etc.\nSo when recently tidying up some files, I found a cool trick for pasting definitions into Quarto files without have to copy and paste it.\n#| echo: false\n#| warnings: false\n#| output: asis\nfrom inspect import getsource as gs\nfrom main import function_to_display\nprint(f'\\n```python\\n{gs(function_to_display)}\\n```\\n')\nJust put that inside a python code chunk and it will show the definition for the imported function."
  },
  {
    "objectID": "content/post/hottest-100/post.html",
    "href": "content/post/hottest-100/post.html",
    "title": "Predicting the 2023 Hottest 100",
    "section": "",
    "text": "Like many Australians, I spent my last Saturday in January getting hyped for the Triple J Hottest 100 countdown. And for the past few years, there has been a project run by 100 Warm Tunas that has been remarkably accurate at predicting the results of the countdown.\nWarm Tunas makes predictions by scraping social media posts for people’s votes and then collating them as a sample of all votes. While this method is highly effective, I feel that it misses the point a bit when it comes to understanding why a song is popular.\nTherefore, this year, I have set out to determine the top songs in the 2023 countdown without relying on anything related to the voting itself."
  },
  {
    "objectID": "content/post/hottest-100/post.html#my-hypotheses",
    "href": "content/post/hottest-100/post.html#my-hypotheses",
    "title": "Predicting the 2023 Hottest 100",
    "section": "My Hypotheses",
    "text": "My Hypotheses\nHeading into this, I have a few ideas as to factors that will make a song perform well in the countdown:\n\nPlays on Triple J\nI feel this factor is pretty self-explanatory. If a song is being played a lot on Triple J, it’s most likely popular with the listener base and will get more votes in the Hottest 100.\n\n\nChart Success\nThis one is a bit weirder, as I don’t think that just getting to number one in the ARIA charts will make you a top pick for Triple J listeners. Otherwise, the countdown would be topped by the year’s biggest pop hits. If a song is too popular in the mainstream, it seems to fall out of favor with Triple J listeners. However, there are some notable exceptions to this, such as “Bad Guy” by Billie Eilish and “Thrift Shop” by Macklemore, which both took out the top spot in their respective years.\n\n\nTime of Release and Peak\nThis idea is commonly thrown around when talking about the Oscars, so I feel that it’s probably going to be applicable to the Hottest 100 as well. Being at peak popularity when people are voting is probably going to be useful. Similarly, a song that hung around for a long time will probably be voted for more than a song that only hung around for a week."
  },
  {
    "objectID": "content/post/hottest-100/post.html#number-of-plays",
    "href": "content/post/hottest-100/post.html#number-of-plays",
    "title": "Predicting the 2023 Hottest 100",
    "section": "Number of Plays",
    "text": "Number of Plays\nTo me, the most obvious indicator of a song’s popularity is the number of plays it receives. So, we can start by examining that.\n\n\n\n\n\n\n\n\n\nThese plots give us a good insight into the trends in how Triple J selects songs. We have a lot of songs with almost no plays, which are mostly songs that are being presented to the audience to gauge their reaction. If they become popular, the songs will be played frequently, indicated by the absence of songs with 40-60 plays. However, very few songs receive excessive playtime, with only a handful surpassing 200 plays.\nWe can also observe the impact of being released early in the year, as these songs have more opportunities to be played throughout the year, resulting in a downward slope for each year."
  },
  {
    "objectID": "content/post/hottest-100/post.html#how-total-plays-impact-success",
    "href": "content/post/hottest-100/post.html#how-total-plays-impact-success",
    "title": "Predicting the 2023 Hottest 100",
    "section": "How Total Plays Impact Success",
    "text": "How Total Plays Impact Success\n\n\n\n\n\n\n\n\n\nLooking at the rankings, we can see that the total number of plays doesn’t have a massive impact on performance. A song can have five plays or a hundred, and it seems to have a similar outcome in the rankings.\nThere is a slight downward trend for songs getting over 120 plays, as these are the absolute most played songs for the year. However, this status still doesn’t guarantee a top spot."
  },
  {
    "objectID": "content/post/hottest-100/post.html#accounting-for-time",
    "href": "content/post/hottest-100/post.html#accounting-for-time",
    "title": "Predicting the 2023 Hottest 100",
    "section": "Accounting for Time",
    "text": "Accounting for Time\nA thought I had while looking at the absolute play data is that it disproportionately rewards songs that were released earlier in the year.\nTo address this, I have compiled some statistics that consider the peak of the songs, which should eliminate any advantage for being released at the beginning of the year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, we can see that there is some useful information, with the peak plays per week showing that songs which have a big peak generally perform well in the final rankings. However, as with the absolute count of plays, there doesn’t seem to be a hard and fast rule."
  },
  {
    "objectID": "content/post/music-reviews/op_ed.html",
    "href": "content/post/music-reviews/op_ed.html",
    "title": "Fixing My Relationship with Spotify",
    "section": "",
    "text": "I grew up in a weird time for music and media in general. I am old enough to\nremember going to JB Hi-Fi to buy CDs and having piles of DVDs next to the TV,\nbut I am also young enough to consider myself a part of the streaming-native\ngeneration. I first got Spotify at age 14, and I remember watching YouTube a few\nyears before that.\nLooking back at what my consumption habits were like when I was\nyounger, I can only think that they have changed for the worse. Instead of\nwatching movies or listening to albums, I now have playlists and the endless\nrecommended videos from YouTube to fill my desires for media.\nI’m scared that this is only going to get worse in the future. TikTok and the UX\nchanges made by Spotify to put playlists front and centre in the experience have\nturned media into something disposable. The library of things to watch or listen\nto is transient, recommended by an algorithm only interested in making you\nconsume more. In short, the enshitification of media and curation has come and\nis here to stay.\nAnd call me a Luddite, but I do have this nostalgic dream of personal curation,\nwhere my library of media is mine to consume as I see fit, rather than curated\nby an algorithm. The growth of records and physical media in the last few years\nshows that this is not a unique want.\nAnd as much as I would love to be the person who has all their films on DVD and\nonly listens to records, the cost, both monetarily and time-wise, makes it out\nof reach for me right now. So instead, I want to work out how to make these\nsystems, most importantly Spotify, work for me in my anti-algorithmic curation\nmindset."
  },
  {
    "objectID": "content/post/music-reviews/op_ed.html#the-passive-vs-the-active-consumer",
    "href": "content/post/music-reviews/op_ed.html#the-passive-vs-the-active-consumer",
    "title": "Fixing My Relationship with Spotify",
    "section": "The Passive vs The Active Consumer",
    "text": "The Passive vs The Active Consumer\nThe first step in this process is to work out what I’m not happy with on Spotify. It’s definitely not the ease of access and the value proposition of 15 dollars a month for pretty much all the music I can consume; it’s great. No, instead I think the issue is best exemplified by looking at my Spotify home screen.\n\n\n\nMy Spotify Home Screen\n\n\nOn it, we have nothing that I have selected myself. Instead, it is all curated playlists that resemble something that I might want to listen to. And on these playlists, as with most of the curated playlists made on Spotify, it’s a hodgepodge of songs that I definitely like, songs that resemble songs that I like, and then some songs that I don’t care for at all but Spotify thinks that I should like. All of these are present in an order that just doesn’t work.\nHowever, because these are front and centre to my experience in Spotify, these are what I listen to. I do get some joy out of them, but it’s a very passive experience. At no point along the way am I deciding what I should be consuming; instead, I just take in what is given.\nThis behaviour is almost antithetical to my experience using iTunes and MP3 players. Here, my curation was active, with the closest thing to a recommendation system being my most played albums section. Everything that I heard was something that I chose because I wanted to listen to it.\nThe divide between active and passive curation is what makes me dislike my\nSpotify experience and nostalgise my time with my MP3 players. I think\nremoving this passive curation process could go a long way to fixing my\nrelationship with Spotify."
  },
  {
    "objectID": "content/post/music-reviews/op_ed.html#curation-as-a-shared-culture",
    "href": "content/post/music-reviews/op_ed.html#curation-as-a-shared-culture",
    "title": "Fixing My Relationship with Spotify",
    "section": "Curation as a shared culture",
    "text": "Curation as a shared culture\nSay I remove my ‘passivley’ created playlists. This presents me with a new issue, how do I find new stuff?\nI have been reliant on Spotify recommending me new songs for the last ten years to flesh out my library. I’m a regular user of my Discover Weekly playlist, and I found some of my favourite artists through their recommendations.\nBut I don’t think that this system is particularly good for embedding me into a shared culture, because it will only recommend things that I will probably like based on my previous listening. That means that when a great album comes out that doesn’t fit into my ‘taste’, it won’t be recommended. When Bad Bunny’s Debí Tirar Más Fotos came out, which is one of my favourite albums of the year so far, it took one of my friends playing it for me to find it. And it’s not like it’s a niche album; it’s currently number one for the albums of the year 2025 on Album of the Year. It’s just that Spotify decided that I wouldn’t like it.\nI really dislike this approach to recommendations taken by Spotify. Let me decide if I don’t like something; let me try something new and out of the box rather than playing it safe. When I want to listen to something safe, I can choose to throw on the Wombats for the hundredth time, but at least that’s my decision.\nThis was something that I really liked about Letterboxd (when it had its five minutes in the spotlight); it exposed me to films that I wouldn’t have had in my Netflix recommendations. The experience of trying something that is recommended by another person rather than a machine contributes to a feeling of sonder, of the shared experience that the media we consume is not unique to ourselves but is part of a wider ecosystem, while the machine-based recommendations further entrench this individualism that is all too prevalent in our hyper-personalised reality."
  },
  {
    "objectID": "content/post/music-reviews/op_ed.html#how-to-make-spotify-more-curatable",
    "href": "content/post/music-reviews/op_ed.html#how-to-make-spotify-more-curatable",
    "title": "Fixing My Relationship with Spotify",
    "section": "How to Make Spotify More Curatable",
    "text": "How to Make Spotify More Curatable\n\nTurn of Auto Play\nThis is a pretty simple one, but it does make a big difference. When I have nothing left in the queue, by default, Spotify will continue playing with recommendations. Turning this off will force me to at least choose what is being listened to.\n\n\nGet Rid of Liked Songs\nI think one of the main culprits of passive curation and consumption is Spotify’s implementation of the liked songs feature. Any time a song is playing, I can click on the + button and it will be added to the ‘liked songs’ playlist. This playlist becomes a dumping ground for all the songs that I like, but if I ever throw it on shuffle, it becomes unlistenable, similar to some of Spotify’s generated playlists.\nInstead, there are two things I will do. The first is liking albums instead of songs. Artists put together albums to be a cohesive piece of work. Picking out songs (or playing them on shuffle) is akin to reading a book and skipping half the chapters and reading them out of order.\nIf I still want playlists, I will make a few instead of dumping songs into liked songs. This makes the act of liking a song more active and will hopefully result in more cohesive playlists.\n\n\nGet Rid of Recommendations\nOn my phone, this is almost impossible; however, defaulting to looking at the library tab instead of home should present me with only things I have decided to add to my library.\nOn the computer, this is best fixed with the help of Spiceify. With it, I can download extensions that will remove recommendations from the client completely.\nA couple I like are:\n\nLibrary\nLuckyLP\nDisable Homepage Recommendations\n\n\n\n\nMy Spotify client\n\n\n\n\nFind New Places for My Recommendations\nThis is me trying to fix the problem of relying on personalised recommendations, so I think that anything that is personalised or ‘AI’ is out of the question. Instead, I’ve really enjoyed looking at reviews on Pitchfork or Album of the Year. Also, shfl has been good tool for random recommendations. This will find me stuff that other people like, but not necessarily specific to me.\nAlso, just asking my friends about what they are listening to has been fun, plus Shazam has been really useful in finding the IDs of the tracks that I hear when I am out and about.\n\nI’m going to try implementing these over the next couple of weeks and see how my relationship changes. I’m hoping to see myself build a better relationship with Spotify and a stronger sense of ownership and control over my music library and habits."
  },
  {
    "objectID": "content/post/tswift-beads/post.html",
    "href": "content/post/tswift-beads/post.html",
    "title": "Maximising the Number of Friendship Bracelets for the Taylor Swift Concert",
    "section": "",
    "text": "With Taylor Swift coming to Melbourne next week, my house has started its prep for the concert. An important part of that preparation is making friendship bracelets to trade at the concert. So we headed down to Spotlight and grabbed ourselves a couple of bags of beads to make the bracelets. However, when we opened them up, we found that the distribution of letters was all over the place. We had a heap of useless Zs while also having almost no vowels. Instead of driving back to Spotlight, I decided to see if I could make enough friendship bracelets from the letters we already had, while also being a bit clever about which songs we were going to make friendship bracelets for."
  },
  {
    "objectID": "content/post/tswift-beads/post.html#first-try",
    "href": "content/post/tswift-beads/post.html#first-try",
    "title": "Maximising the Number of Friendship Bracelets for the Taylor Swift Concert",
    "section": "First Try",
    "text": "First Try\nI set out to make an algorithm to determine the best set of song titles we could use. I want to assign each song title a cost, and then make the song with the lowest cost the bracelets. I can keep doing this until I can’t make any more bracelets. To determine the cost of a song title, I just summed the costs of its letters. The cost of the letters was the number of occurrences it had in the list of songs divided by the number of beads I had remaining for that letter.\n\nfrom collections import defaultdict\nimport re\n\ndef count_chars_in_list(list_of_strings):\n    char_counts = defaultdict(int)\n    \n    for string in list_of_strings:\n        for char in string:\n            char_counts[char] += 1\n    \n    return dict(char_counts)\n\n\ndef bracelets(song_list, bead_dict):\n\n    # W and M is interchangeable\n    cleaned_dict = {\n        re.sub(r'\\W+', '', i.lower()).replace(\"w\",\"m\"): i \n        for i in song_list\n    }\n\n    # dict of letter usage totals\n    letter_popularity = count_chars_in_list(cleaned_dict.keys())\n\n    # will run until a break is called\n    while True:\n        cost_dict = {}\n        # Determine the cost of all the songs\n        for song in cleaned_dict.keys():\n            song_cost = 0\n            flag = False\n            for char in song: \n                # Assign the cost by adding up beads value\n                if char in bead_dict:\n                    # If no beads for that letter are left then that word cant be formed\n                    if bead_dict[char] == 0:\n                        flag = True\n                        break\n                    else:\n                        song_cost += letter_popularity[char]/bead_dict[char]\n            if not flag:\n                cost_dict[song] = song_cost\n\n        # Finish loop if no more songs can be made\n        if len(cost_dict) ==0:\n            break\n        # Find the cheapest song\n        cost_dict_sorted = list(dict(sorted(cost_dict.items(), key=lambda item: item[1])).keys())\n        cheapeast_song = cost_dict_sorted[0]\n        print(cleaned_dict[cheapeast_song])\n        # Remove the cheapest songs beads from the bead counts\n        for char in cheapeast_song:\n            bead_dict[char] -= 1\nbracelets(song_list.copy(), bead_dict.copy())\n\nivy\nivy\nivy\nivy\nivy\nivy\nivy\nivy\nivy\nRun\nSlut\nME\nSlut\nivy\nME\nRun\nhoax\nSlut\nME\nhoax\nGlitch\nRun\nivy\nME\nhoax\nGlitch\nhoax\nME\nGlitch\nKarma\n\n\nThis was pretty good, but let’s remove the repeated songs because I don’t want to have 10 bracelets with Ivy on them. We can do this by adding del cleaned_dict[cheapest_song] to the end of the loop.\n\ndef bracelets(song_list, bead_dict):\n    cleaned_dict = {re.sub(r'\\W+', '', i.lower()).replace(\"w\",\"m\"): i for i in song_list}\n    letter_popularity = count_chars_in_list(cleaned_dict.keys())\n\n    while True:\n        cost_dict = {}\n        for song in cleaned_dict.keys():\n            song_cost = 0\n            flag = False\n            for char in song: \n                # Assign the cost by adding up beads value\n                if char in bead_dict:\n                    if bead_dict[char] == 0:\n                        flag = True\n                        break\n                    else:\n                        song_cost += letter_popularity[char]/bead_dict[char]\n            if not flag:\n                cost_dict[song] = song_cost\n        if len(cost_dict) ==0:\n            break\n        cost_dict_sorted = list(dict(sorted(cost_dict.items(), key=lambda item: item[1])).keys())\n        cheapeast_song = cost_dict_sorted[0]\n        print(cleaned_dict[cheapeast_song])\n        for char in cheapeast_song:\n            bead_dict[char] -= 1\n        del cleaned_dict[cheapeast_song]\n\n\nbracelets(song_list.copy(), bead_dict.copy())\n\nivy\nRun\nSlut\nME\nGlitch\nhoax\nMine\nwillow\nParis\naugust\nMidnights\nRed\nMean\nOurs\nDaylight\nInvisible\nLondon Boy"
  },
  {
    "objectID": "content/post/tswift-beads/post.html#getting-picky",
    "href": "content/post/tswift-beads/post.html#getting-picky",
    "title": "Maximising the Number of Friendship Bracelets for the Taylor Swift Concert",
    "section": "Getting Picky",
    "text": "Getting Picky\nI presented this list to my housemates only to get the response, ‘I hate ME!’ So, I did some cleaning to remove some of the so-called ‘banned songs’. It also turns out that I’m not allowed to listen to “London Boy” anymore since the guy it is about is canceled or something? Not sure, but now we have a new list that doesn’t include the songs we don’t want.\n\nbannded_songs = [\n\"Invisible\",\n\"London Boy\",\n\"ME\",\n'hoax',\n'run'\n]\n\nbracelets([song for song in song_list if song not in bannded_songs], bead_dict.copy())\n\nivy\nRun\nSlut\nGlitch\nwillow\nMine\nParis\naugust\nMean\nOurs\nMidnights\nClean\nStyle\ngold rush\nDaylight\nLong Live"
  },
  {
    "objectID": "content/post/tswift-beads/post.html#a-final-go",
    "href": "content/post/tswift-beads/post.html#a-final-go",
    "title": "Maximising the Number of Friendship Bracelets for the Taylor Swift Concert",
    "section": "A Final Go",
    "text": "A Final Go\nI tried showing this list, which received a better reception, but there were still a couple of non-negotiable songs that needed to be included. We also decided that the Qs and the Os look close enough to be interchangeable, so I changed the way we generate the cleaned dict to reflect that.\n\ndef bracelets(song_list, bead_dict):\n    cleaned_dict = {re.sub(r'\\W+', '', i.lower()).replace(\"w\",\"m\").replace(\"q\",'o'): i for i in song_list}\n    letter_popularity = count_chars_in_list(cleaned_dict.keys())\n\n    while True:\n        cost_dict = {}\n        for song in cleaned_dict.keys():\n            song_cost = 0\n            flag = False\n            for char in song: \n                # Assign the cost by adding up beads value\n                if char in bead_dict:\n                    if bead_dict[char] == 0:\n                        flag = True\n                        break\n                    else:\n                        song_cost += letter_popularity[char]/bead_dict[char]\n            if not flag:\n                cost_dict[song] = song_cost\n        if len(cost_dict) ==0:\n            break\n        cost_dict_sorted = list(dict(sorted(cost_dict.items(), key=lambda item: item[1])).keys())\n        cheapeast_song = cost_dict_sorted[0]\n        print(cleaned_dict[cheapeast_song])\n        for char in cheapeast_song:\n            bead_dict[char] -= 1\n        del cleaned_dict[cheapeast_song]\n\n\n# Move all the Q beads to O\nbead_dict['o'] += bead_dict['q']\ndel bead_dict['q']\n\nrequired_songs = ['Delicate', 'Lover']\n    \nfor song in required_songs:\n    print(song)\n    for char in re.sub(r'\\W+', '', song.lower()).replace(\"w\",\"m\").replace(\"q\",'o'):\n        bead_dict[char] -= 1\n\nbracelets([song for song in song_list if song not in bannded_songs and song not in required_songs], bead_dict.copy())\n\nDelicate\nLover\nivy\nwillow\nRun\nSlut\nGlitch\nOurs\nBad Blood\naugust\nMine\nParis\nMidnights\nLong Live\nLast Kiss\n\n\nAnd there’s a final list of 14 bracelets we can make with our current beads. Would it have been faster to drive back to Spotlight to buy more beads? Probably, but this was more fun."
  },
  {
    "objectID": "content/post/taylor-series/post-1.html",
    "href": "content/post/taylor-series/post-1.html",
    "title": "Using Taylor Series to Improve the Euler Method",
    "section": "",
    "text": "Euler’s method is a classic way of approximating first-order differential equations. In short, it uses the derivative of a function and starting condition to estimate the value of the function a short distance from the starting point.\nThis is commonly written as:\n\\[\n\\frac{dy}{dx} = f(x, y)\n\\] \\[\ny(x+h) = y(x) + hf(x, y(x)) + \\epsilon\n\\] \\[\n\\lim_{h \\to 0} |\\epsilon| = 0\n\\]\nWhere \\(\\epsilon\\) is the error created by the approximation.\n\n\nGeneralizing Euler’s method to higher order ODEs is pretty easy. All you have to do is think of the ODE as a vector with each entry being the next derivative of the function. You can now write Euler’s Method in terms of this function:\n\\[\n\\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n-1}(x+h)\\\\\n\\end{bmatrix} = \\begin{bmatrix}\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n-1}(x)\\\\\n\\end{bmatrix} + \\begin{bmatrix}\nh & 0 & 0 &  ... & 0\\\\\n0 & h & 0 &  ... & 0\\\\\n0 & 0 & h &  ... & 0\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & h\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\ny'(x)\\\\\ny''(x)\\\\\ny'''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix} + \\epsilon\n\\]\nOr shifting the \\(Y'\\) matrix to make it a bit prettier:\n\\[\\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} =  \\begin{bmatrix}\n1 & h & 0 &  ... & 0\\\\\n0 & 1 & h &  ... & 0\\\\\n0 & 0 & 1 &  ... & 0\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix} + \\epsilon \\]"
  },
  {
    "objectID": "content/post/taylor-series/post-1.html#higher-order-odes",
    "href": "content/post/taylor-series/post-1.html#higher-order-odes",
    "title": "Using Taylor Series to Improve the Euler Method",
    "section": "",
    "text": "Generalizing Euler’s method to higher order ODEs is pretty easy. All you have to do is think of the ODE as a vector with each entry being the next derivative of the function. You can now write Euler’s Method in terms of this function:\n\\[\n\\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n-1}(x+h)\\\\\n\\end{bmatrix} = \\begin{bmatrix}\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n-1}(x)\\\\\n\\end{bmatrix} + \\begin{bmatrix}\nh & 0 & 0 &  ... & 0\\\\\n0 & h & 0 &  ... & 0\\\\\n0 & 0 & h &  ... & 0\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & h\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\ny'(x)\\\\\ny''(x)\\\\\ny'''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix} + \\epsilon\n\\]\nOr shifting the \\(Y'\\) matrix to make it a bit prettier:\n\\[\\begin{bmatrix}\ny(x+h)\\\\\ny'(x+h)\\\\\ny''(x+h)\\\\\n...\\\\\ny^{n}(x+h)\\\\\n\\end{bmatrix} =  \\begin{bmatrix}\n1 & h & 0 &  ... & 0\\\\\n0 & 1 & h &  ... & 0\\\\\n0 & 0 & 1 &  ... & 0\\\\\n... & ... & ... &  ... & ...\\\\\n0 & 0 & 0 &  ... & 1\\\\\n\\end{bmatrix} \\cdot \\begin{bmatrix}\ny(x)\\\\\ny'(x)\\\\\ny''(x)\\\\\n...\\\\\ny^{n}(x)\\\\\n\\end{bmatrix} + \\epsilon \\]"
  },
  {
    "objectID": "content/post/taylor-series/post-4.html",
    "href": "content/post/taylor-series/post-4.html",
    "title": "Optimising the settings for the ODE solver",
    "section": "",
    "text": "In the last post in this series, I introduced my method for approximating ODEs. But after re-rereading it, I found myself questioning the step where I convert the ODE into a matrix.\nThis is best exemplified by the ODE \\(y'' = x + y\\) which goes through the point \\(y(0) = 1\\) and \\(y'(x) = 1\\). Converting it into a matrix, we would define \\(T\\) as:\n\\[\nT_1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 1 & 1 & 0 & 0\\\\\n\\end{bmatrix}\n\\]\nHowever, rearranging this ODE into \\(x = y'' - y\\) or \\(y = y'' - x\\), it would be just as reasonable to define T as:\n\\[\nT_2 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & -1 & 0 & 1\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\] \\[\nT_3 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & -1 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nSo does this matter? Since they are all equaly valid ways of representing the ODE, surley they will all lead to the same conclusion. However Using each of these matrices the ODE we get very different solutions. \\(T1\\) is the closet, \\(T3\\) and \\(T2\\) are both equally far off."
  },
  {
    "objectID": "content/post/taylor-series/post-4.html#truncation-error",
    "href": "content/post/taylor-series/post-4.html#truncation-error",
    "title": "Optimising the settings for the ODE solver",
    "section": "Truncation Error",
    "text": "Truncation Error\nTruncation error is the error that is introduced by approximating a infinite series, such as the Taylor series we use, with a finite number of terms. I’m suspecting in this case, that \\(T_1\\) introduces the least amount of truncation error at each step and this is why it is the best aproximation of the ODE.\nWe can define our truncation error (\\(R\\)) as the difference be the true value (\\(Y^*\\)) and the predicted value (\\(Y\\)). In our aproximation, this is defined as:\n\\[R = Y^*(x+h) - Y(x+h) = Y^*(x+h) - T \\cdot S \\cdot Y(x) \\]\nWhere \\(S\\) is the stepping matrix and \\(T\\) is the transformation matrix.\nSince our approximation is based on the Taylor series, the truncation error is the remaining terms in the series that aren’t used. When \\(k\\) terms of the taylor series are used, the truncation error (\\(r\\)) is :\n\\[ r = \\sum_{n = k+2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n\\]\nSince \\(y^{n}\\) be pulled out as a constant for a given step, the limit of the magnitude of this error can be defined using big-O notation, such that when \\(k\\) terms are used, \\(r\\) is \\(O(h^{k+3})\\).\nLooking at just the stepping matrix, we can see the truncation error is:\n\\[S = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & h & \\frac{h^2}{2}\\\\\n0 & 0 & 0 & 1 & h\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix} \\rightarrow R =  \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\\]\nTaking it back to the above example of \\(y'' = x + y\\), and the three transformation matrices it defines, we can apply the transformation matrices to the vector \\(R\\) of the standard stepping matrix to get the truncation error for each of the approximations.\n\\[T_1 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h^3)\\\\\n\\end{bmatrix}\n\\]\n\\[T_2 \\cdot R = \\begin{bmatrix}\n0\\\\\nO(h)\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n\\]\n\\[T_3 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n\\]\nNow looking at the magnitude of these vectors:\n\\[ |T_1 \\cdot R | = \\sqrt{(O(h^3))^2 + (O(h^2))^2 + (O(h^3))^2} = O(h^2) \\] \\[ |T_2 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^3))^2 + (O(h^2))^2 + (O(h))^2}  = O(h) \\] \\[ |T_3 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^2))^2 + (O(h))^2} $ = O(h) \\]\nThis shows that \\(|T_1 \\cdot R |\\) shrinks as \\(h\\) gets smaller faster than when you use \\(T_2\\) or \\(T_3\\). More generally, you can say that \\(T_1\\) will lead to the least error.\nNow these are the errors for each step, so now looking at the truncation error for the whole approximation, we can multiply the error by the number of steps.\nSay we want to find the value for for the curve at \\(x=1\\), we would need to take \\(1/h\\) steps to find this value.\n\\[ |T_1 \\cdot R | \\cdot \\frac{1}{h} = O(h) \\] \\[ |T_2 \\cdot R | \\cdot \\frac{1}{h} = O(1) \\] \\[ |T_3 \\cdot R | \\cdot \\frac{1}{h} = O(1) \\]\nWe can now test this by running some approximations with various values of h and the different matrices, the results for which are below. We can see that the \\(T_1\\) result follows the \\(O(h)\\) curve while the \\(T_2\\) and \\(T_3\\) result follows the \\(O(1)\\) curve.\n\n\n\n\n\n\n\n\n\nSo now trying to generalise this beyond the example we’ve worked through, the reduction in error using the \\(T_1\\) matrix was caused by redefining the term with the most truncation error, \\(y''\\) with terms with less truncation error, \\(y\\) and \\(x\\). Both \\(T_2\\) and \\(T_3\\) failed to do this.\nSo in general, to minimise error we should try to define the highest derivative in terms of the lower terms, as this will remove the \\(O(h)\\) error from the truncation vector."
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html",
    "href": "content/post/drug-use-models/final_report.html",
    "title": "Modeling Drug Use in Communities",
    "section": "",
    "text": "This was written as a University project but I’m pretty happy with how it turned out.\nBig thanks to Kevin Dai and Chrysovalantis Thomopoulos who worked with me on this throughout the semester."
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#initial-model-sir",
    "href": "content/post/drug-use-models/final_report.html#initial-model-sir",
    "title": "Modeling Drug Use in Communities",
    "section": "Initial Model: SIR",
    "text": "Initial Model: SIR\nIt’s common to liken drug use to disease, treating drug addiction as an infection that can be recovered from. Using this analogy, we began looking into how diseases are modeled and how we can adapt our own model to fit drug usage. One of the simplest drug models is the SIR model, a compartment model based on the concept of mass action that sees infections as caused by interactions between infected people and susceptible people. The use of mass action can be justified in this case as we are looking at large populations over a long period of time, meaning that the variance in the movement will average out over time. There are a few reasons that this model is a good starting point for any type of drug modeling: Firstly, this model is very simple and can be easily adapted to our own needs with extra compartments and transferable conditions. Secondly, since we are looking at drug use in a population, relying on mass action rather than discrete modeling allows us to further simplify the model. Thirdly, the idea that addiction is caused by interactions between individuals seems to be a good assumption for drug use as one would expect people to become familiar and subsequently addicted to certain substances by being associated with people who have them.\nOne divergence we made from the most basic SIR model is allowing for relapse from the recovered group back into the infected group. This represents how recovering addicts will become addicted again at a different rate than people who have never used the drug before. However, an important fact is that people cannot move back into the susceptible group as addiction is said to last a lifetime.\n\n\n\nSIR Diagram\n\n\nOur SIR model is defined by the following equations: \\[\\frac{dS}{dt} = -\\pi_1 * S * I \\] \\[\\frac{dI}{dt} = \\pi_1 * S * I + \\pi_2 * R * I - \\pi_3 *  I \\] \\[\\frac{dR}{dt} = -\\pi_2 * R * I + \\pi_3 * I \\]\nThe parameters \\(\\pi_1\\), \\(\\pi_2\\), and \\(\\pi_3\\) represent the ‘infectiousness’ of the drug for susceptible and recovering individuals and the rate of recovery from the drug, respectively.\nWe now look at some plots to see how this model behaves for various combinations of our parameters:\n\n\n/tmp/ipykernel_3974/1601081733.py:3: DeprecationWarning: The catppuccin-matplotlib package is deprecated, please upgrade to https://github.com/catppuccin/python (pip install catppuccin)\n\n\n  import mplcatppuccin"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#suar-model",
    "href": "content/post/drug-use-models/final_report.html#suar-model",
    "title": "Modeling Drug Use in Communities",
    "section": "SUAR Model",
    "text": "SUAR Model\nThe SIR model fails to distinguish between different severities of addiction. By grouping the individuals who are trying/testing the drug with people who are addicted, we fail to capture the behavior of people trying the drug without becoming reliant on it. Because of this, we adapted the SIR model, relabeled the infected group to addicted, and added in a new compartment model for users of the drug who are not addicted, the ‘using’ group. Susceptible people will now move into the using group before moving into the Addicted group. Since there are now two groups who are using the drug and interacting with the community, the \\(I\\) terms of the ODEs in the SIR model have to be replaced with \\(U+A\\).\n\n\n\nSUAR Diagram\n\n\nThe new model can now be described by the following equations:\n\\[\\frac{dS}{dt} = - \\pi_1*S*(A+U) + \\pi_2*U   \\] \\[\\frac{dU}{dt} =  \\pi_1*S*(A+U) - \\pi_2*U  - \\pi_3*U \\] \\[\\frac{dA}{dt} = \\pi_3*U +\\pi_5 *(A+U)*R - \\pi_4*A \\] \\[\\frac{dR}{dt} =  \\pi_4*A - \\pi_5 *(A+U)*R \\]\nWith parameters \\(\\pi_1\\), \\(\\pi_2\\), \\(\\pi_3\\), \\(\\pi_4\\), and \\(\\pi_5\\) being the ‘effectiveness’ of the drug for susceptible people, the rate that people stop trying the drug, the rate at which people become addicted, the recovery rate for addicted people respectively, and the ‘effectiveness’ for recovered people."
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#saur-model-with-age",
    "href": "content/post/drug-use-models/final_report.html#saur-model-with-age",
    "title": "Modeling Drug Use in Communities",
    "section": "SAUR Model with age",
    "text": "SAUR Model with age\nNow focusing on the stationary points of the previous two models (see appendix for derivation), we realize that our long-term behavior involves the extinction of either the user and/or the addicted group. However, this behavior is unrealistic and rather too optimistic since there is a consistent number of people using and addicted to the drug, and a consistently high number of people who are susceptible to the drug. To fix this issue, we integrate age groups into the model, grouping the population into children, teens, young adults, adults, and seniors. Each of these groups has a distinct rate of death, and the births are proportional to the number of young adults and adults. Separating the population also allows us to set different parameters according to each age group, capturing more nuanced behaviors such as an increased volume of experimentation of young people which would not have been identified otherwise.\nAnother feature we wanted to tackle is how people recover. In the same way that people start using drugs based on interactions with others who are using them, we expect people to recover based on interactions with people who aren’t using them. This can be represented by support groups, concerned parents, friends, etc. Furthermore, we can expect people in one age group and compartment model to be influenced differently from each age group and subsequent compartment model. We can store this information with a matrix \\(I\\), where \\(I_{ij}\\) is the magnitude of the influence that group j has on group i.\nWe can now go in to further simplify this by defining two new terms, the positive and negative influence on a age group \\(i\\), to be: \\[P_i = \\sum{k=1}{5}{I_{ik} * (S_k + R_K)}\\] \\[N_i = \\sum{k=1}{5}{I_{ik} * (U_k + A_K)}\\]\nThe model now has 20 compartments with both sideways movement from and to age groups and downwards movement as people age.\nLooking across age group ‘i’, the model can be described as:\n\\[\\frac{dS_i}{dt} = aS_{i-1} + aU_{i-1} \\pi_{i-1,2}P_{i-1} - S_i\\pi_{i,1}N_i + (1-a)U_i\\pi_{i,2}P_i - aSi - d_{i,s}S_i + (b\\sum_{k=2}^{3}S_k+U_k+A_k+R_K | i = 0)  \\] \\[\\frac{dU_i}{dt} =  aS_{i-1}\\pi_{i-1,1}N_{i-1} + aU_{i-1} - U_i * \\pi_{i,3} - a U_i - U_i\\pi_{i,2}P_i + (1-a)S_i\\pi_{i,1}N_i - d_{i,U}U_i\\] \\[\\frac{dA_i}{dt} = aU_{i-1}\\pi_{i-1,3} + aA_{i-1} + aR_{i-1}\\pi_{i-1,5}N_{i-1} - A_i\\pi_{i,4}P_i + (1-a)R_i\\pi_{i,5}N_{i}-aA_i + (1-a)U_i\\pi_{i,3}- d_{i,A}A_i\\] \\[\\frac{dR_i}{dt} = aA_{i-1}\\pi_{i-1,4}P_{i-1} + aR_{i-1} - aR_i -R_i\\pi_{i,5}N_i + (1-a)A_i\\pi_{i,4}P_i - d_{i,R}R_I \\]\nWith the parameters being sorted in matrices \\(\\pi\\), {d} and {i}, containing row-wise versions of the parameters for the SAUR model, the death rate for each compartment, and the influence that each age group has on the other, and scalars \\(a\\) and \\(b\\), which define the aging rate and the birth rate of the population."
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#simplified-suar-model-with-age",
    "href": "content/post/drug-use-models/final_report.html#simplified-suar-model-with-age",
    "title": "Modeling Drug Use in Communities",
    "section": "Simplified SUAR Model with age",
    "text": "Simplified SUAR Model with age\nAfter creating the SAUR model with age groups, we experimented with various combinations of parameters to see how the model reacted. However, we found that with 20 compartments and 73 parameters, it is really hard to go through all the combinations to find meaningful results from the model. We could see that there were stationary points for the population portions, but finding them analytically was near impossible. Because of this, we sought to find a way to simplify the model to produce results that are easier to understand. We did this by removing the different age groups altogether, while keeping the equations describing the model the same. By removing the age groups, the equations dramatically simplify into this form;\n\n\n\nSUAR Diagram with ageing\n\n\n\\[\\frac{dS}{dt} = -S\\pi_{1}N +U\\pi_2P  -d_SS + b(S+U+A+R)  \\] \\[\\frac{dU}{dt} = S\\pi_1N -U\\pi_2P - U\\pi_3 - d_UU  \\] \\[\\frac{dA}{dt} = U\\pi_3 - A\\pi_4P + R\\pi_5N - d_AA\\] \\[\\frac{dR}{dt} = A\\pi_4P - R\\pi_5N - d_RR \\]\n\n\n\n\n\n\n\n\n\nLooking at this model, it is clear to see that we will only find stationary points when births and deaths are equal. However we also want to see long term trends in growing and shrinking populations. To do this, we look at this model through population portions, redefining the system as:\n\\[ T = S + U + A + R \\] \\[ T' = b(S+U+A+R)  -d_SS -d_UU - d_AA - d_RR \\]\n\\[ \\frac{d\\frac{S}{N}}{dt} = \\frac{(-S\\pi_{1}N +U\\pi_2P  -d_SS + b(S+U+A+R))T-T'S}{T^2} \\] \\[ \\frac{d\\frac{U}{N}}{dt} = \\frac{(S\\pi_1N -U\\pi_2P - U\\pi_3 - d_UU )T-T'U}{T^2} \\] \\[ \\frac{d\\frac{A}{N}}{dt} = \\frac{(U\\pi_3 - A\\pi_4P + R\\pi_5N - d_AA)T-T'A}{T^2}  \\] \\[ \\frac{d\\frac{R}{N}}{dt} = \\frac{(A\\pi_4P - R\\pi_5N - d_RR )T-T'R}{T^2} \\]\nDenoting these fractions as s,u,a,r and reformatting the equation using T as a characteristic for population and \\(\\frac{1}{b}\\) as a characteristic for time;\n\\[ s+u+a+r = 1\\] \\[ \\frac{ds}{dt} = (-s\\pi_{1}(u+a)+u\\pi_2(s+r)  -d_Ss + 1)-s(1 - d_Ss - d_Uu - d_Aa - d_Rr) \\] \\[ \\frac{du}{dt} = (s\\pi_{1}(u+a)-u\\pi_2(s+r) - u\\pi_3 -d_Uu )-u(1 - d_Ss - d_Uu - d_Aa - d_Rr) \\] \\[ \\frac{da}{dt} = (u\\pi_3 -a\\pi_4(s+r) + r\\pi_5(u+a) -d_Aa )-a(1 - d_Ss - d_Uu - d_Aa - d_Rr)  \\] \\[ \\frac{dr}{dt} = (a\\pi_4(s+r) - r\\pi_5(u+a) -d_Rr )-r(1 - d_Ss - d_Uu - d_Aa - d_Rr)\\]"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-1-stationary-points-of-sir-model",
    "href": "content/post/drug-use-models/final_report.html#appendix-1-stationary-points-of-sir-model",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 1 : Stationary points of SIR model",
    "text": "Appendix 1 : Stationary points of SIR model\n\\[0 = -\\pi_1 * S * I \\] \\[0 = \\pi_1 * S * I + \\pi_2 * R * I - \\pi_3 *  I \\] \\[0 = -\\pi_2 * R * I + \\pi_3 * I \\]\nTherefore either \\(\\pi_1\\), \\(S\\) or \\(I\\) must be 0\nIf \\(I = 0\\):\n\\[0 = -\\pi_1 * S * 0 \\] \\[0 = \\pi_1 * S * 0 + \\pi_2 * R * 0 - \\pi_3 *  0 \\] \\[0 = -\\pi_2 * R * 0 + \\pi_3 * 0 \\]\nTherefore any point [\\(S\\),0,\\(R\\)] is stationary.\nIf \\(\\pi_1 = 0\\) or \\(S=0\\):\n\\[0 =  0 * I \\] \\[0 = 0 * I + \\pi_2 * R * I - \\pi_3 *  I \\] \\[0 = -\\pi_2 * R * I + \\pi_3 * I \\]\nTherefore:\n\\[ \\pi_2 * R * I - \\pi_3 *  I = - \\pi_2 * R * I + \\pi_3 *  I  \\] \\[ \\pi_2 * R * I = \\pi_3 * I \\] \\[ R:I = \\pi_3:\\pi_2 \\]\nTherefore any point [0, \\(\\frac {P_{total}*\\pi_2}{\\pi_2 + \\pi_3}\\) ,\\(\\frac {P_{total}*\\pi_3}{\\pi_2 + \\pi_3}\\)]"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-2-stationary-points-of-suar",
    "href": "content/post/drug-use-models/final_report.html#appendix-2-stationary-points-of-suar",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 2 : Stationary points of SUAR",
    "text": "Appendix 2 : Stationary points of SUAR\n\\[0 = - \\pi_1*S*(A+U) + \\pi_2*U   \\] \\[0 =  \\pi_1*S*(A+U) - \\pi_2*U  - \\pi_3*U \\] \\[0 = \\pi_3*U +\\pi_4 *(A+U)*R - \\pi_5*A \\] \\[0 =  \\pi_5*A - \\pi_4 *(A+U)*R \\]\nBy combing the first two and last two equations, one can see that if the parameters are non 0, \\(U=0\\). Therefore:\n\\[ 0 = \\pi_1*S*A \\] \\[0 = \\pi_4 *A*R - \\pi_5*A \\] \\[0 =  \\pi_5*A - \\pi_4 *A*R \\]\nTherefore \\(S = 0\\) or \\(A = 0\\) In the case \\(A = 0\\), any point [\\(S\\),0,0,\\(R\\)] will be stationary\nIn the case \\(S=0\\) \\[ \\pi_4*A*R = \\pi_5*A\\] \\[R:A = \\pi_5:\\pi_4\\]\nTherefore any point [0,0, \\(\\frac {P_{total}*\\pi_4}{\\pi_4 + \\pi_5}\\) ,\\(\\frac {P_{total}*\\pi_5}{\\pi_4 + \\pi_5}\\)]"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-3-critical-points-of-simplified-suar-model",
    "href": "content/post/drug-use-models/final_report.html#appendix-3-critical-points-of-simplified-suar-model",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 3: Critical Points of Simplified SUAR model",
    "text": "Appendix 3: Critical Points of Simplified SUAR model\nWhen s = 1: \\[ s = 1\\] \\[ \\frac{ds}{dt} = (-d_Ss + 1)-s(1 - d_Ss) = (1-S)(1-d_Ss)  = 0 \\] \\[ \\frac{du}{dt} = 0 \\] \\[ \\frac{da}{dt} = 0  \\] \\[ \\frac{dr}{dt} = 0  \\]\nWhen s+r = 1: \\[ s+r = 1\\] \\[ s = 1 - r \\]\n\\[ \\frac{ds}{dt} = (-d_Ss + b)-s(1 - d_Ss - d_Rr)  = (1 -s)*(1 + d_r*s - d_s*s)\\] \\[ \\frac{du}{dt} = 0 \\] \\[ \\frac{da}{dt} = 0  \\] \\[ \\frac{dr}{dt} = (-d_Rr )-r(1 - d_Ss - d_Rr) = (s - 1)*(1 + d_r*s - d_s*s)\\]"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-4-matlab-code-for-generating-conditional-stationary-points",
    "href": "content/post/drug-use-models/final_report.html#appendix-4-matlab-code-for-generating-conditional-stationary-points",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 4: Matlab code for generating conditional Stationary Points",
    "text": "Appendix 4: Matlab code for generating conditional Stationary Points\nsyms s u a r p1 p2 p3 p4 p5 ds du da dr\n\n\node1 =  (-s*p1*(u+a)+u*p2*(s+r)  -ds*s + 1)-s *(1 - ds*s - du*u - da*a - dr*r);\node2 =  (s*p1*(u+a)-u*p2*(s+r) - u*p3 -du*u )-u*(1 - ds*s - du*u - da*a - dr*r) ;\node3 =  (u*p3 -a*p4*(s+r) + r*p5*(u+a) -da*a )-a*(1 - ds*s - du*u - da*a - dr*r);\node4 =  (a*p4*(s+r) - r*p5*(u+a) -dr*r )-r*(1 - ds*s -du*u - da*a - dr*r);\nsums = s+u+a+r;\n\n\n\nodes = [ode1 == 0, ode2 == 0, ode3 == 0 , ode4 == 0, sums  == 1, s&gt;=0, u &gt;= 0, a &gt;=0, r&gt;=0];\n\n\nss = [];\navg = [];\n\nk = 3;\n\nfor p1i = -k:k\nfor p2i   = -k:k\nfor p3i  = -k:k\nfor p4i  = -k:k\nfor p5i = -k:k\n[p1, p2, p3, p4, p5, ds, du, da, dr] = deal(2^p1i, 2^p2i, 2^p3i, 2^p4i, 2^p5i,1,1,1,1);\nsol = solve(subs(odes),'Real',true);\nss = [ss ; [p1i,p2i,p3i,p4i,p5i,length(sol.s) - 1]];\nif length(sol.s) &gt; 1\navg = [avg ; [p1i,p2i,p3i,p4i,p5i,mean(sol.u(2:length(sol.u)) + sol.a(2:length(sol.a)))]];\nend;end;end;end;end;end\n\n\n\nwritematrix(double(avg),'avg.csv')\nwritematrix(double(ss), 'ss.csv')"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-5-python-code-for-simulating-sir-model",
    "href": "content/post/drug-use-models/final_report.html#appendix-5-python-code-for-simulating-sir-model",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 5: Python code for simulating SIR model",
    "text": "Appendix 5: Python code for simulating SIR model\n\ndef SIR(p1, p2, p3, S0, I0, R0, i):\n    vect_initial = np.array([S0, I0, R0]).T\n\n    output = pd.DataFrame(columns=[\"day\", \"S\", \"I\", \"R\"])\n    j = 0\n    vect = vect_initial.copy()\n\n    while j &lt; i:\n        vectplus1 = np.zeros([3])\n        S = vect[0]\n        I = vect[1]\n        R = vect[2]\n        vectplus1[0] = S - p1 * S * I\n        vectplus1[1] = I + p1 * S * I + p2 * R * I - p3 * I\n        vectplus1[2] = R - p2 * R * I + p3 * I\n        output.loc[j] = np.concatenate((np.array([j]), vectplus1.T))\n        vect = vectplus1.copy()\n        j += 1\n\n    plt.plot(output[\"day\"], output[\"S\"])\n    plt.plot(output[\"day\"], output[\"I\"])\n    plt.plot(output[\"day\"], output[\"R\"])\n    plt.title(\"p1=\" + str(p1) + \", p2=\" + str(p2) + \", p3=\" + str(p3))\n    plt.legend([\"S\", \"I\", \"R\"])"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-6-python-code-for-simulating-suar-model",
    "href": "content/post/drug-use-models/final_report.html#appendix-6-python-code-for-simulating-suar-model",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 6: Python code for simulating SUAR model",
    "text": "Appendix 6: Python code for simulating SUAR model\n\ndef SUAR(p1, p2, p3, p4, p5, S0, U0, A0, R0, i):\n    vect_initial = np.array([S0, U0, A0, R0]).T\n\n    output = pd.DataFrame(columns=[\"day\", \"S\", \"U\", \"A\", \"R\"])\n    j = 0\n    vect = vect_initial.copy()\n\n    while j &lt; i:\n        vectplus1 = np.zeros([4])\n        S = vect[0]\n        U = vect[1]\n        A = vect[2]\n        R = vect[3]\n        vectplus1[0] = S - p1 * S * (A + U) + p2 * U\n        vectplus1[1] = U + p1 * S * (A + U) - p2 * U - p3 * U\n        vectplus1[2] = A + p3 * U + p5 * (A + U) * R - p4 * A\n        vectplus1[3] = R + p4 * A - p5 * (A + U) * R\n        output.loc[j] = np.concatenate((np.array([j]), vectplus1.T))\n        vect = vectplus1.copy()\n        j += 1\n\n    plt.plot(output[\"day\"], output[\"S\"])\n    plt.plot(output[\"day\"], output[\"U\"])\n    plt.plot(output[\"day\"], output[\"A\"])\n    plt.plot(output[\"day\"], output[\"R\"])\n    plt.title(\n        \"p1=\"\n        + str(p1)\n        + \", p2=\"\n        + str(p2)\n        + \", p3=\"\n        + str(p3)\n        + \", p4=\"\n        + str(p4)\n        + \", p5=\"\n        + str(p5)\n    )\n    plt.legend([\"S\", \"U\", \"A\", \"R\"])"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-7-python-code-for-simulating-20-compartment-suar-model",
    "href": "content/post/drug-use-models/final_report.html#appendix-7-python-code-for-simulating-20-compartment-suar-model",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 7: Python code for simulating 20 compartment SUAR model",
    "text": "Appendix 7: Python code for simulating 20 compartment SUAR model\n\ndef SUAR_20(aging_rate, birth_rate, death_rate, I, P, X0, steps):\n    C = pd.DataFrame(columns=[\"s\", \"t\", \"a\", \"r\"], dtype=\"float128\")\n    T = pd.DataFrame(columns=[\"s\", \"t\", \"a\", \"r\"], dtype=\"float128\")\n    Y = pd.DataFrame(columns=[\"s\", \"t\", \"a\", \"r\"], dtype=\"float128\")\n    A = pd.DataFrame(columns=[\"s\", \"t\", \"a\", \"r\"], dtype=\"float128\")\n    S = pd.DataFrame(columns=[\"s\", \"t\", \"a\", \"r\"], dtype=\"float128\")\n\n    (n_age_groups, n_status) = np.shape(X0)\n    itterations = 0\n    while itterations &lt; steps:\n        C.loc[itterations] = X0[0]\n        T.loc[itterations] = X0[1]\n        Y.loc[itterations] = X0[2]\n        A.loc[itterations] = X0[3]\n        S.loc[itterations] = X0[4]\n\n        i = 0\n\n        IE = np.matmul(I, X0)\n\n        Xn = np.zeros(np.shape(X0))\n\n        while i &lt; n_age_groups:\n            group_vect = X0[i].copy()\n            s = group_vect[0]\n            t = group_vect[1]\n            a = group_vect[2]\n            r = group_vect[3]\n            interaction_neg = np.sum(IE[i][[1, 2]])\n            interaction_pos = np.sum(IE[i][[0, 3]])\n\n            Xn[i][0] = (\n                s - (s * P[i][0] * interaction_neg) + t * P[i][1] * interaction_pos\n            )\n            Xn[i][1] = (\n                t\n                + (s * P[i][0] * interaction_neg)\n                - t * P[i][1] * interaction_pos\n                - t * P[i][2]\n            )\n            Xn[i][2] = (\n                a\n                + t * P[i][2]\n                + r * P[i][4] * interaction_neg\n                - a * P[i][3] * interaction_pos\n            )\n            Xn[i][3] = r - r * P[i][4] * interaction_neg + a * P[i][3] * interaction_pos\n\n            i += 1\n\n        Xn_aged = np.zeros(np.shape(X0))\n\n        i = 0\n        j = 0\n\n        while i &lt; n_age_groups:\n            j = 0\n            while j &lt; n_status:\n                if i == 0:\n                    if j == 0:\n                        Xn_aged[i][j] = Xn[i][j] * (\n                            1 - aging_rate - death_rate[i][j]\n                        ) + birth_rate * sum(Xn[2] + Xn[3])\n                j += 1\n            i += 1\n\n        itterations += 1"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-8-python-code-for-simulating-5-compartment-suar-model",
    "href": "content/post/drug-use-models/final_report.html#appendix-8-python-code-for-simulating-5-compartment-suar-model",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 8: Python code for simulating 5 compartment SUAR model",
    "text": "Appendix 8: Python code for simulating 5 compartment SUAR model\n\ndef SUAR_5(p1, p2, p3, p4, p5, b, d, S0, U0, A0, R0, i):\n    vect_initial = np.array([S0, U0, A0, R0]).T\n\n    output = pd.DataFrame(columns=[\"day\", \"S\", \"U\", \"A\", \"R\"])\n    j = 0\n    vect = vect_initial.copy()\n\n    while j &lt; i:\n        vectplus1 = np.zeros([4])\n        S = vect[0]\n        U = vect[1]\n        A = vect[2]\n        R = vect[3]\n        vectplus1[0] = (\n            S - p1 * S * (A + U) + p2 * U * (S + R) - d[0] * S + b * (S + U + A + R)\n        )\n        vectplus1[1] = U + p1 * S * (A + U) - p2 * U * (S + R) - p3 * U - d[1] * U\n        vectplus1[2] = A + p3 * U + p5 * (A + U) * R - p4 * A * (R + S) - d[2] * A\n        vectplus1[3] = R + p4 * A * (S + R) - p5 * (A + U) * R - d[3] * R\n\n        output.loc[j] = np.concatenate((np.array([j]), vectplus1.T))\n        vect = vectplus1.copy()\n        j += 1\n\n    output[\"pop\"] = output[\"S\"] + output[\"U\"] + output[\"A\"] + output[\"R\"]\n    plt.plot(output[\"day\"], output[\"S\"] / output[\"pop\"])\n    plt.plot(output[\"day\"], output[\"U\"] / output[\"pop\"])\n    plt.plot(output[\"day\"], output[\"A\"] / output[\"pop\"])\n    plt.plot(output[\"day\"], output[\"R\"] / output[\"pop\"])\n    plt.title(\n        \"Portion: p1=\"\n        + str(p1)\n        + \", p2=\"\n        + str(p2)\n        + \", p3=\"\n        + str(p3)\n        + \", p4=\"\n        + str(p4)\n        + \", p5=\"\n        + str(p5)\n    )\n    plt.legend([\"S\", \"U\", \"A\", \"R\"])"
  },
  {
    "objectID": "content/post/drug-use-models/final_report.html#appendix-9-python-code-for-simulating-5-compartment-dimensionless-suar-model",
    "href": "content/post/drug-use-models/final_report.html#appendix-9-python-code-for-simulating-5-compartment-dimensionless-suar-model",
    "title": "Modeling Drug Use in Communities",
    "section": "Appendix 9: Python code for simulating 5 compartment dimensionless SUAR model",
    "text": "Appendix 9: Python code for simulating 5 compartment dimensionless SUAR model\n\ndef SUAR_norm(p1, p2, p3, p4, p5, d, S0, U0, A0, R0, i, h, plot=True):\n    vect_initial = np.array([S0, U0, A0, R0]).T\n\n    output = pd.DataFrame(columns=[\"day\", \"s\", \"u\", \"a\", \"r\"])\n    j = 0\n    vect = vect_initial.copy()\n\n    ds = d[0]\n    du = d[1]\n    da = d[2]\n    dr = d[3]\n\n    while j &lt; i:\n        vectplus1 = np.zeros([4])\n        s = vect[0]\n        u = vect[1]\n        a = vect[2]\n        r = vect[3]\n        vectplus1[0] = s + h * (\n            (-s * p1 * (u + a) + u * p2 * (s + r) - ds * s + 1)\n            - s * (1 - ds * s - du * u - da * a - dr * r)\n        )\n        vectplus1[1] = u + h * (\n            (s * p1 * (u + a) - u * p2 * (s + r) - u * p3 - du * u)\n            - u * (1 - ds * s - du * u - da * a - dr * r)\n        )\n        vectplus1[2] = a + h * (\n            (u * p3 - a * p4 * (s + r) + r * p5 * (u + a) - da * a)\n            - a * (1 - ds * s - du * u - da * a - dr * r)\n        )\n        vectplus1[3] = r + h * (\n            (a * p4 * (s + r) - r * p5 * (u + a) - dr * r)\n            - r * (1 - ds * s - du * u - da * a - dr * r)\n        )\n        output.loc[j] = np.concatenate((np.array([j * h]), vectplus1.T))\n        vect = vectplus1.copy()\n        j += 1\n    if plot:\n        plt.plot(output[\"day\"], output[\"s\"])\n        plt.plot(output[\"day\"], output[\"u\"])\n        plt.plot(output[\"day\"], output[\"a\"])\n        plt.plot(output[\"day\"], output[\"r\"])\n        plt.title(\n            \"Portion: p1=\"\n            + str(p1)\n            + \", p2=\"\n            + str(p2)\n            + \", p3=\"\n            + str(p3)\n            + \", p4=\"\n            + str(p4)\n            + \", p5=\"\n            + str(p5)\n        )\n        plt.legend([\"S\", \"U\", \"A\", \"R\"])\n        return\n    else:\n        return [s, u, a, r]"
  },
  {
    "objectID": "content/drafts/3-ODE-solver-implementation/ODE-solve.html",
    "href": "content/drafts/3-ODE-solver-implementation/ODE-solve.html",
    "title": "Taylor Series approximations of ODEs: The Algorithm",
    "section": "",
    "text": "import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef c(a, cond, n):\n    if n == 0:\n        return cond;\n    if n == 1:\n        return a * math.sin(cond)\n    return cond\n\n\nint_x = 0\nint_y = 2\n# 2\ndepth = 2\nstep = 0.001\nd_min = -3\nd_max = 3\n\n# 2.1 Set up output space\nd_min_scale = int(round(d_min / step))\nd_max_scale = int(round(d_max / step))\nls_points = list(range(d_min_scale, d_max_scale))\nls_points = list(map(lambda x: x * step, ls_points))\ndf = pd.DataFrame(ls_points, columns=['Point'])\ndf['Value'] = 0\ndf.set_index('Point', inplace=True)\n\n# 2.2 Set up n list\nls_n = list(range(0, depth))\n\n\n# 3\ndef point(step, vector, ls_n, derivative):\n    ls_point_term = []\n    for n in ls_n:\n        coefficient = c(vector, n + derivative)\n        point = coefficient / math.factorial(n) * step ** n\n        ls_point_term.append(point)\n    point_value = sum(ls_point_term)\n    return point_value\n\n\na = int_x\ncond = int_y\nwhile a &lt;= d_max:\n    point_value = point(step, a, cond, ls_n)\n    location = df.index.get_loc(a, method='nearest')\n    df.iloc[location, 0] = point_value\n    cond = point_value\n    a += step\n\na = int_x\ncond = int_y\n\nwhile a &gt;= d_min:\n    point_value = point(-step, a, cond, ls_n)\n    location = df.index.get_loc(a, method='nearest')\n    df.iloc[location, 0] = point_value\n    cond = point_value\n    a -= step\n\ndf.plot()\nplt.show()\n\n\nimport math\nimport numpy as np\nimport itertools as it\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n## Set parameters for approximation\nh = 0.001\nxmin = -math.pi * 2\nxmax = math.pi * 2\n\n## Int vector such that [h,x,f(x),f'(x) .... f^n(x)]\nint_list = np.array([h, math.pi / 2, -5 / 3, np.NAN, 15])\nint_vect = int_list.reshape(len(int_list), 1)\n\n## Condition Vector\ncondition_vect = np.array([[h, 0, np.NAN, 5, np.NAN],\n                          [h, 2, 0, np.NAN, np.NAN],\n                          [h, 0, np.NAN, 0, np.NAN]])\nsearch_min = 1.43\nsearch_max = 1.45\nsearch_step = 0.01\n\n## Find Permutaions\nsearch_permutations = [ *it.permutations(np.arange(search_min, search_max, search_step), np.count_nonzero(np.isnan(int_vect)))]\nnan_location = np.argwhere(np.isnan(int_vect))\nnan_location = nan_location[nan_location != 0]\nposs_int_vect = np.zeros((len(search_permutations), len(int_vect)))\nfor i in range(0, len(poss_int_vect)):\n    poss_int_vect[i] = int_vect.T\n    poss_int_vect[i][nan_location] = search_permutations[i]\n\n## Set up matrix for step definition\nstep_matrix_pos = np.identity(len(int_vect))\nstep_matrix_neg = np.identity(len(int_vect))\n\n## Set up x stepping system\nstep_matrix_pos[1][0] = 1\nstep_matrix_neg[1][0] = -1\n\n## Set up taylor seies h values\nfor i in range(2, len(step_matrix_pos)):\n    for j in range(i, len(step_matrix_pos)):\n        step_matrix_pos[i][j] = h ** (j - i) / math.factorial(j - i)\n\n## Set up taylor seies h/n! values\nfor i in range(2, len(step_matrix_neg)):\n    for j in range(i, len(step_matrix_neg)):\n        step_matrix_neg[i][j] = ((-h) ** (j - i)) / math.factorial(j - i)\n\n\n## Swap rows of step_matrix to represent linear ODE\ndef linear_step_converion(matrix):\n    matrix[4] = -9 * matrix[2]\n    return matrix\n\n\nstep_matrix_pos = linear_step_converion(step_matrix_pos)\nstep_matrix_neg = linear_step_converion((step_matrix_neg))\n\n\n## Create adjusting Function for non linear ODE\ndef adjustment(vector):\n    vector[4] = -9 * vector[2] + math.cos(vector[1])\n    return vector\n\n\n## Run Simulation for all permutations, returning the distance from the condition\n\nls_distance = []\nfor int_vect in poss_int_vect:\n    ## Create Output Space\n    x_list = np.arange(xmin, xmax, h)\n\n    output = np.zeros((len(x_list), len(int_vect)))\n    output[0:len(output), 0] = h\n    output[0:len(output), 1] = x_list\n    ## Set up stepping process\n\n    ## Do the positive steps first\n    vect = int_vect.copy()\n    while vect[1] &lt;= xmax + h:\n        index = int((vect[1] - xmin) / h)\n        try:\n            output[index] = vect.T\n        except IndexError:\n            pass\n        vect = np.matmul(step_matrix_pos, vect)\n        vect = adjustment(vect)\n\n    ## Do the negative steps next\n    vect = int_vect.copy()\n    while vect[1] &gt; xmin:\n        index = int((vect[1] - xmin) / h)\n        try:\n            output[index] = vect.T\n        except IndexError:\n            pass\n        vect = np.matmul(step_matrix_neg, vect)\n        vect = adjustment(vect)\n\n    ## Check condition\n    dist_ls = []\n    for vect in condition_vect:\n        index = int(math.floor((vect[1] - xmin) / h))\n        dist = np.linalg.norm(np.nan_to_num(output[index] - vect))\n        dist_ls.append(dist)\n    dist_sum = sum(dist_ls)\n    ls_distance.append(dist_sum)\n\ndf_results = pd.DataFrame(poss_int_vect)\ndf_results['dist'] = ls_distance\n\nbest_index = df_results[['dist']].idxmin()\n\nbest_int_vect = poss_int_vect[best_index]\n\n## Create Output Space\nx_list = np.arange(xmin, xmax, h)\n\noutput = np.zeros((len(x_list), len(int_vect)))\noutput[0:len(output), 0] = h\noutput[0:len(output), 1] = x_list\n## Set up stepping process\n\n## Do the positive steps first\nvect = best_int_vect.T.copy()\nwhile vect[1] &lt;= xmax + h:\n    index = int((vect[1] - xmin) / h)\n    try:\n        output[index] = vect.T\n    except IndexError:\n        pass\n    vect = np.matmul(step_matrix_pos, vect)\n    vect = adjustment(vect)\n\n## Do the negative steps next\nvect = best_int_vect.T.copy()\nwhile vect[1] &gt; xmin:\n    index = int((vect[1] - xmin) / h)\n    try:\n        output[index] = vect.T\n    except IndexError:\n        pass\n    vect = np.matmul(step_matrix_neg, vect)\n    vect = adjustment(vect)\n\ndist_ls = []\nfor vect in condition_vect:\n    index = int(math.floor((vect[1] - xmin) / h))\n    dist = np.linalg.norm(np.nan_to_num(output[index] - vect))\n    dist_ls.append(dist)\ndist_sum = sum(dist_ls)\n\nplt.plot(output.T[1], output.T[2])\nplt.title(str(best_int_vect))\nplt.show()\n\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n## Varibles to set to define simultaion\ndef f(x):  ## Only for plotting against estimation\n    return\n\ndef c(n):  ## C vaules\n    if n == 0:\n        return 2\n    if n == 1:\n        return 3\n    if n == 2:\n        return 3 + math.sin(2) + 3\n\n\na = 0  ## Around what point is this estimation\nstep = 0.1  ## step Size for points\nwidth = 10  ## Range of Estimation\ndepth = 3  ## Max size of n\n\n## Create List of points\nwidth_mult = 1 / step\nnew_witdh = int(round(width * width_mult))\nls_points = range(-new_witdh, new_witdh)\nls_points = list(map(lambda x: x * step, ls_points))\n\n## Create List n\nls_n = list(range(0, depth))\n\n## Create List of coefficients\nls_c = list(map(lambda x: c(x), ls_n))\n\n\ndef point_term(x, a, cond, n):\n    point = cond / math.factorial(n) * (x - a) ** n\n    return point\n\n\ndf = pd.DataFrame(ls_points, columns=['Point'])\n\n## Create A datframe with each collumn being the vaules for a single n\nfor n in ls_n:\n    df[n] = df['Point'].apply(lambda x: point_term(x, a, ls_c[n], n))\n\ndf.set_index('Point', inplace=True)\n\n## Sum all the estimations\ndf_estimation = df.sum(axis=1)\n\ndf_estimation.plot(label='Estimation')\n\nif f(a):\n    plt.plot(ls_points, list(map(lambda x: f(x), ls_points)), label='Actual', linestyle='dashed', )\nplt.legend()\n\nplt.show()"
  }
]
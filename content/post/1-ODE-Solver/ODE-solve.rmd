---
title: "Taylor Series approximations of ODEs: The Algorithm"
author: Alfie Chadwick
date: 2021-05-18T00:00:00+00:00
Tags: ['Maths', 'Differential Equations']
output:
  blogdown::html_page:
    toc: true
---

If you ever start doing university level maths, physics, chemistry or even economics, you will most likely start running into differential equations. These are equations that describe functions in terms of their derivatives. Normally in these classes, at least at the start, all the problems are solvable by hand with some fairly rudimentary algebra and calculus. However, if you ever try and work with these problems in the real world, you will quickly realise that a lot of differential equations aren't solvable.

This is where numerical methods become useful as they provide one the ability to approximate the solution of the differential equation without explicitly solving it.

## Euler's Method
One of the first numerical methods that most people get taught is Euler's method. There are many [better writers](https://tutorial.math.lamar.edu/classes/de/eulersmethod.aspx) who can explain this method much better than me, so I won't go into too much detail. Essentially, this method uses the definition of the differential equation to calculate steps of a given length h. For an ODE (differential equation of one variable) of order n, the method can be described as:

$$ \begin{bmatrix}
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n-1}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n-1}(x)\\
\end{bmatrix} + \begin{bmatrix}
h & 0 & 0 &  ... & 0\\
0 & h & 0 &  ... & 0\\
0 & 0 & h &  ... & 0\\
... & ... & ... &  ... & ...\\
0 & 0 & 0 &  ... & h\\
\end{bmatrix} \cdot \begin{bmatrix}
y'(x)\\
y''(x)\\
y'''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$

This can also be written as:

$$ \begin{bmatrix}
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
1 & h & 0 &  ... & 0\\
0 & 1 & h &  ... & 0\\
0 & 0 & 1 &  ... & 0\\
... & ... & ... &  ... & ...\\
0 & 0 & 0 &  ... & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$


### How to expand the Euler method?
The Euler Method approximates functions as straight lines at each point and to then evaluate the function at the next point. However, if the function is not much like a straight line then this approximation will fail to produce accurate results. Therefore, the question becomes how could one take into account the curve of the function between steps.

## Taylor Series
A solution to the problem posed above is to use a taylor series of the function, which describes a function as a polynomial determined by its derivatives. Explicitly, it is defined as :

$$ y(x) =  \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n}(a)}{n!}\cdot(x - a)^n $$

Where $a$ is the point where the approximation is built.


When looking at the point $a + h$ exclusively, this formula transforms to:

$$ y(a+h) = \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n}(a)}{n!}\cdot(h)^n $$


A nice property of the taylor series is that it has a really simple derivative function:

$$ y'(a+h) = \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n+1}(a)}{n!}\cdot(h)^n $$
$$ y^m(a+h) = \sum_{n = 0}^{\infty}  \frac{\mathbb y^{n+m}(a)}{n!}\cdot(h)^n $$

For a given h, this function describes a linear combination of the derivatives of a function, which can then be writen as a matrix multiplication

$$ \begin{bmatrix}
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^n}{n!}\\
0 & 1 & \frac{h}{1!} &  ... & \frac{h^{n-1}}{(n-1)!}\\
0 & 0 & 1 &  ... & \frac{h^{n-2}}{(n-2)!}\\
... & ... & ... &  ... & ...\\
0 & 0 & 0 &  ... & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
y'(x)\\
y''(x)\\
y'''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$

One can see that this step matrix is similar to the matrix that defines the steps of the Euler method, however the Euler method doesn't take into account each of the higher derivatives, whereas this new step matrix does.

Now expanding the input vector to contain x and h information;

$$ \begin{bmatrix}
h\\
x+h\\
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 &  ... & 0\\
1 & 1 & 0 & 0 & 0 &  ... & 0\\
0 & 0 & 1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^n}{n!}\\
0 & 0 & 0 & 1 & \frac{h}{1!} &  ... & \frac{h^{n-1}}{(n-1)!}\\
0 & 0 & 0 & 0 & 1 &  ... & \frac{h^{n-2}}{(n-2)!}\\
... & ... & ... & ... & ... &  ... & ...\\
0 & 0 & 0 & 0 & 0 &  ... & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
h\\
x\\
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$

For the sake of simplicity, this equation can referred to as $Y(x+h) = S \cdot Y(x)$

## How to make the approximation fit the ODE
The current $Y(x+h) = S \cdot Y(x)$ descirbes the path of a polynominal through space. To use it to approximate an ODE, it must be slightly modified such that for each step of the aproximation, the condition of the ODE is met.
This pert of the problem has to be turned into two separate problems, one for ODEs that are exclusively linear combinations of the input vector and one they are not.

#### Case 1 : Exclusively linear

For an ODE that is a linear combination of x, y and y's derivatives, such as $y''=x+y$, on can simply change multiply the function by a matrix representing this linear combination. This can be written as:

$$T \cdot Y(x+h) = T \cdot S \cdot Y(x) $$


Knowing that each row of the stepping matrix represents  one of h, x, y or y's derivatives, creating a transformation matrix is fairly easy.

Sticking with the same sample ODE $y''=x+y$, the transformation matrix can be defined as;

$$
T_1 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 1 & 1 & 0 & 0\\
\end{bmatrix}
$$

However, thinking of this ODE as $x = y'' - y$  and $y = y'' - x$ it is just as reasonable to define T as:

$$
T_2 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 0 & -1 & 0 & 1\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

or

$$
T_3 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & -1 & 0 & 0 & 1\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

So what is best option for the approximation. It's not yet obvious, so lets look at the resulting step matrices from these transformations $T \cdot S$

$$
T_1 \cdot S= \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & h & \frac{h^2}{2}\\
0 & 0 & 0 & 1 & h\\
1 & 1 & 1 & h & \frac{h^2}{2}\\
\end{bmatrix}
$$

$$
T_2 \cdot S= \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 0 &  -1 & -h & 1-\frac{h^2}{2}\\
0 & 0 & 1 & h & \frac{h^2}{2}\\
0 & 0 & 0 & 1 & h\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

$$
T_3 \cdot S= \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & 0\\
-1 & -1 & 0 & 0 & 1\\
0 & 0 & 0 & 1 & h\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

Now we have to look at an important part of approximations, truncation error, that is, assuming the function has a solution $y^*$, what is the magnitude of the difference between this function and the approximation. Formally, this is said to be $r = y^* - y$.  Since 5 curves are being simulated at the same time, this remainder still be seen as a vector R with the definition:

$$R = Y^*(x+h) - Y(x+h) = Y^*(x+h) - S \cdot Y(x) $$

Since this approximation is based on the taylor series, the truncation error is the remaining terms in the approximation. For an approximation with the greatest term k, the truncation error is therefore:

$$ r = \sum_{n = k+1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n$$

For the standard stepping matrix for the order two ODE:

$$\begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & h & \frac{h^2}{2}\\
0 & 0 & 0 & 1 & h\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix} \rightarrow R =  \begin{bmatrix}
0\\
0\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
$$

Since this is the error for an un-transformed system, by simply applying the transformation matrix $R$ we can find the truncation error for the transformed system.

$$T_1 \cdot R = \begin{bmatrix}
0\\
0\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
 0 + \sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}$$

$$T_2 \cdot R = \begin{bmatrix}
0\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n-\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
\mathbb y'(x)\cdot(h) + \frac{\mathbb y''(x)}{2}\cdot(h)^2\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}$$


$$T_3 \cdot R  = \begin{bmatrix}
0\\
0\\
0-\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}$$

Although the magnitude of these  errors are theoretically possible to compute, it's a lot of work considering that the question we are trying to answer is which one is the smallest. One tool we can employ to simplify this problem is big-O notation. Big-O notation is defined as $f(x) = O(g(x))$  as $x \rightarrow a$ if $|f(x)| = M \cdot g(x)$ . It describes the upper limit of a function as it's variable approaches a. In this case, we are interested in the upper limit as h approaches 0.

Applying this concept to the R vectors:

$$T_1 \cdot R  = \begin{bmatrix}
0\\
0\\
O(h^3)\\
O(h^2)\\
O(h^3)\\
\end{bmatrix}
$$

$$T_2 \cdot R = \begin{bmatrix}
0\\
O(h)\\
O(h^3)\\
O(h^2)\\
O(h)\\
\end{bmatrix}
$$

$$T_3 \cdot R  = \begin{bmatrix}
0\\
0\\
O(h)\\
O(h^2)\\
O(h)\\
\end{bmatrix}
$$

Now looking at the magnitude of these vectors:


$$ |T_1 \cdot R | = \sqrt{(O(h^3))^2 + (O(h^2))^2 + (O(h^3))^2} $$
$$ |T_2 \cdot R | = \sqrt{(O(h))^2 + (O(h^3))^2 + (O(h^2))^2 + (O(h))^2} $$
$$ |T_3 \cdot R | = \sqrt{(O(h))^2 + (O(h^2))^2 + (O(h))^2} $$

Since $f_1 \cdot f_2 = O(g_1g_2)$;

$$ |T_1 \cdot R |  = \sqrt{O(h^6) + O(h^4) + O(h^6)} $$
$$ |T_2 \cdot R |  = \sqrt{O(h^2) + O(h^6) + O(h^4) + O(h^2)} $$
$$ |T_3 \cdot R |  = \sqrt{O(h^2) + O(h^4) + O(h^2)} $$

Since $f_1 + f_2 = O(max(g_1,g_2))$;

$$ |T_1 \cdot R | = \sqrt{O(h^4)} $$
$$ |T_2 \cdot R | = \sqrt{O(h^2)} $$
$$ |T_3 \cdot R | = \sqrt{O(h^2)} $$

Therefore:

$$ |T_1 \cdot R | = O(h^2) $$
$$ |T_2 \cdot R | = O(h) $$
$$ |T_3 \cdot R | = O(h) $$

This shows that $|R_1|$ has the lowest upper limit of the 3 vectors, showing that $R_1$ is the smallest remainder matrix and therefore $T_1$ is the best transformation matrix for approximating the ODE.

Now looking more generally for an ODE of order k, the stepping matrix, the remainder matrix and the big-O  remainder matrix is ;

$$
S = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 &  ... & 0\\
1 & 1 & 0 & 0 & 0 &  ... & 0\\
0 & 0 & 1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^k}{k!}\\
0 & 0 & 0 & 1 & \frac{h}{1!} &  ... & \frac{h^{k-1}}{(k-1)!}\\
0 & 0 & 0 & 0 & 1 &  ... & \frac{h^{k-2}}{(k-2)!}\\
... & ... & ... & ... & ... &  ... & ...\\
0 & 0 & 0 & 0 & 0 &  ... & 1\\
\end{bmatrix}
$$
$$
 R =  \begin{bmatrix}
0\\
0\\
\sum_{n = k+1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = k}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = k-1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
...\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
 =  \begin{bmatrix}
0\\
0\\
O(h^{k+1})\\
O(h^{k})\\
O(h^{k-1})\\
...\\
O(h)
\end{bmatrix}
$$

$$ |R| = O(h) $$

Therefore, to determine the best transformation matrix for a given ODE, you want to define the element that gives the $O(h)$ remainder as a combination of smaller remainder terms as this will then convert the approximation to $O(h^2)$. Therefore, to optimise the transformation matrix, wherever possible, define higher order derivatives as functions of lower order derivatives, x and y, and where this is not possible, leave it as is.



#### Case 2: Everything Else

For non-linear cases, no transformation matrix will match the conditions set out in the ODE. Therefore, an adjustment function , $A(x)$has to be defined to match the conditions such that:

$$Y(x+h) =  A(S \cdot Y(x))$$

Taking the example ODE $Y = xY''$, we have a similar problem to before where three adjustment functions can be defined such that $$Y \leftarrow xY''$$   $$Y'' \leftarrow \frac{Y}{x}$$  $$x \leftarrow \frac{Y}{Y''}$$

To now look at which one gives the best approximation one can take a similar action to previously, by applying the adjustment function to the remainder function

$$ A_1(\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
x_5\\
\end{bmatrix})
=
\begin{bmatrix}
x_1\\
x_2\\
x_2 \cdot x_5\\
x_4\\
x_5\\
\end{bmatrix}
$$

$$ A_2(\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
x_5\\
\end{bmatrix})
=
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
x_3 / x_2\\
\end{bmatrix}
\forall   x_2 \neq 0
or
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
0\\
\end{bmatrix}
x_2 = 0
$$

$$ A_1(\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
x_5\\
\end{bmatrix})
=
\begin{bmatrix}
x_1\\
x_3/x_5\\
x_3\\
x_4\\
x_5\\
\end{bmatrix}
\forall   x_5 \neq 0
$$


Now applying these functions to the remainder vector;

$$A_1(R) = \begin{bmatrix}
0\\
0\\
0\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
$$

$$A_2(R) =
\begin{bmatrix}
0\\
0\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
0\\
\end{bmatrix}
$$

$$A_3(R) = \begin{bmatrix}
0\\
\frac{\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\}{\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\}\\
\sum_{n = 3}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\sum_{n = 1}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n\\
\end{bmatrix}
$$


Converting again to the big-O notation;

$$A_1(R) = \begin{bmatrix}
0\\
0\\
0\\
O(h^2)\\
O(h)\\
\end{bmatrix}
$$

$$A_2(R) =
\begin{bmatrix}
0\\
0\\
O(h^3)\\
O(h^2)\\
0\\
\end{bmatrix}
$$

$$A_3(R) = \begin{bmatrix}
0\\
O(h^2)\\
O(h^3)\\
O(h^2)\\
O(h)\\
\end{bmatrix}
$$

And then taking the magnitude:

$$ |A_1(R)| = O(h) $$
$$ |A_2(R)| = O(h^2) $$
$$ |A_3(R)| = O(h) $$

Therefore, $A_2(x)$ is the best adjustment function for this ODE. Unlike the exclusively linear case there is no hard and fast rule to get the best adjustment function, however similarly to the linear case you want to remove the highest order term, but the adjustment functions will need to be checked to determine the best one.


## The Final Algorithm
#### Linear Case
1) For an ODE of order k, and a step size h convert the initial condition into a vector Y(x) such that:
$$Y(x) = \begin{bmatrix}
h\\
x\\
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{k}(x)\\
\end{bmatrix}$$

2) Define an initial stepping function such that:
$$ \begin{bmatrix}
h\\
x+h\\
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 &  ... & 0\\
1 & 1 & 0 & 0 & 0 &  ... & 0\\
0 & 0 & 1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^n}{n!}\\
0 & 0 & 0 & 1 & \frac{h}{1!} &  ... & \frac{h^{n-1}}{(n-1)!}\\
0 & 0 & 0 & 0 & 1 &  ... & \frac{h^{n-2}}{(n-2)!}\\
... & ... & ... & ... & ... &  ... & ...\\
0 & 0 & 0 & 0 & 0 &  ... & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
h\\
x\\
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$

3) Define a transformation matrix to apply to Y(x) such that Y(x) always satisfies the conditions of the ODE, and that minimises truncation error.

4) Define the new stepping function such that:
$$ \begin{bmatrix}
h\\
x+h\\
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = T \cdot \begin{bmatrix}
1 & 0 & 0 & 0 & 0 &  ... & 0\\
1 & 1 & 0 & 0 & 0 &  ... & 0\\
0 & 0 & 1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^n}{n!}\\
0 & 0 & 0 & 1 & \frac{h}{1!} &  ... & \frac{h^{n-1}}{(n-1)!}\\
0 & 0 & 0 & 0 & 1 &  ... & \frac{h^{n-2}}{(n-2)!}\\
... & ... & ... & ... & ... &  ... & ...\\
0 & 0 & 0 & 0 & 0 &  ... & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
h\\
x\\
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix}$$

5) Iterate this stepping function over the desired domain.


#### General case
1) Same as the linear case

2) Same as linear case

3) Define a transformation **function** to apply to Y(x) such that Y(x) always satisfies the conditions of the ODE, and that minimises truncation error.

4) Define a new stepping matrix such that:
$$ \begin{bmatrix}
h\\
x+h\\
y(x+h)\\
y'(x+h)\\
y''(x+h)\\
...\\
y^{n}(x+h)\\
\end{bmatrix} = A(\begin{bmatrix}
1 & 0 & 0 & 0 & 0 &  ... & 0\\
1 & 1 & 0 & 0 & 0 &  ... & 0\\
0 & 0 & 1 & \frac{h}{1!} & \frac{h^2}{2!} &  ... & \frac{h^n}{n!}\\
0 & 0 & 0 & 1 & \frac{h}{1!} &  ... & \frac{h^{n-1}}{(n-1)!}\\
0 & 0 & 0 & 0 & 1 &  ... & \frac{h^{n-2}}{(n-2)!}\\
... & ... & ... & ... & ... &  ... & ...\\
0 & 0 & 0 & 0 & 0 &  ... & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
h\\
x\\
y(x)\\
y'(x)\\
y''(x)\\
...\\
y^{n}(x)\\
\end{bmatrix})$$

5) Iterate this stepping function over the desired domain.

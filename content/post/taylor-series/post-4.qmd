---
title: "Optimising the settings for the ODE solver"
author: 'Alfie Chadwick'
date: '2024-3-15'
lastmod: "`r Sys.Date()`"
Tags: ['Calculus', 'Algebra']
---

```{python example-setup, echo = FALSE}
import numpy as np
import math
import matplotlib.pyplot as plt
import numpy as np
import mplcatppuccin
import matplotlib as mpl
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
mpl.style.use("macchiato")

def expanded_euler(dims, h):
    step_matrix = np.zeros((dims, dims))
    for i in range(dims):
        for j in range(i, dims):
            # Is 1, and h at j-i =0, 1 respectively
            step_matrix[i, j] = h ** (j - i) / math.factorial(j - i)
    expanded_matrix = add_x_and_1(step_matrix, h)
    return expanded_matrix


def add_x_and_1(original_matrix, h):
    new_size = len(original_matrix) + 2
    new_matrix = np.zeros((new_size, new_size), dtype=original_matrix.dtype)
    # Set the 2x2 top left matrix
    new_matrix[0:2, 0:2] = [[1, 0], [h, 1]]
    # Copy the original matrix to the bottom right of the new matrix.
    new_matrix[2:, 2:] = original_matrix
    return new_matrix



class Solution:
    def __init__(self, input_list: list):
        solution_list = sorted(input_list, key=lambda x: x[1])
        dims = len(solution_list[0]) - 2
        self.x = np.array([x[1] for x in input_list])
        value_lists = [[] for _ in range(dims)]
        for v in input_list:
            for i in range(dims):
                value_lists[i].append(v[i + 2])
        for i in range(dims):
            self.__dict__[f"y_{i}"] = np.array(value_lists[i])
    def interpolate(self, x, y_n):
        """
        allows you to get any value from the solution by interpolating the points

        """
        y_values = self.__dict__[f"y_{y_n}"]
        x_max_index = np.where(self.x >= x)[0][0]
        x_min_index = np.where(self.x <= x)[0][-1]
        x_at_x_max = self.x[x_max_index]
        x_at_x_min = self.x[x_min_index]
        y_at_x_max = y_values[x_max_index]
        y_at_x_min = y_values[x_min_index]
        slope = (y_at_x_max - y_at_x_min) / (x_at_x_max - x_at_x_min)
        value = y_at_x_min + slope * (x - x_at_x_min)
        return value

def linear(y, step_matrix_generator, transformation_matrix, steps=10, h=0.1):
    dims = len(y) - 2
    step_matrix = transformation_matrix @ step_matrix_generator(dims, h)
    output_list = []
    y_n = y.copy()
    i = 0
    while i < steps:
        y_n = step_matrix @ y_n
        output_list.append(y_n)
        i += 1
    return Solution(output_list)
```

In the [last post](/2024/01/12/making-my-ode-solver-solve-odes/) in this series, I introduced my method for approximating ODEs. But after re-rereading it, I found myself questioning the step where I convert the ODE into a matrix.

This is best exemplified by the ODE $y'' = x + y$ which goes through the point $y(0) = 1$ and $y'(x) = 1$. Converting it into a matrix, we would define $T$ as:

$$
T_1 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 1 & 1 & 0 & 0\\
\end{bmatrix}
$$

However, rearranging this ODE into  $x = y'' - y$  or $y = y'' - x$, it would be just as reasonable to define T as:

$$
T_2 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 0 & -1 & 0 & 1\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$
$$
T_3 = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & -1 & 0 & 0 & 1\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

So does this matter? Since they are all equaly valid ways of representing the ODE, surley they will all lead to the same conclusion. However Using each of these matrices the ODE we get very different solutions. $T1$ is the closet, $T3$ and $T2$ are both equally far off.

```{python example, echo = FALSE}
init_y = [1,0,1,1,1] #[1,x,y,y']
T1 = np.array([
    [1, 0, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, 0, 1, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 1, 1, 0, 0]
])

T2 = np.array([
    [1, 0, 0, 0, 0],
    [0, 0, -1, 0, 1],
    [0, 0, 1, 0, 0],
    [0, 0, 0, 1, 0],
    [0, 0, 0, 0, 1]
])

T3 = np.array([
    [1, 0, 0, 0, 0],
    [0, 1, 0, 0, 0],
    [0, -1, 0, 0, 1],
    [0, 0, 0, 1, 0],
    [0, 0, 0, 0, 1]
])
steps = 10

solution1 = linear(
    init_y,
    expanded_euler,
    T1,
    steps=steps, h=1/steps)
    
solution2 = linear(
    init_y,
    expanded_euler,
    T2,
    steps=2*steps, h=-1/steps)
solution3 = linear(
    init_y,
    expanded_euler,
    T3,
    steps=steps, h=1/steps)


plt.plot(solution1.x, solution1.y_0, label='T1 Aproximated Solution')
plt.plot(solution2.x, solution2.y_0, label='T2 Aproximated Solution')
plt.plot(solution3.x, solution3.y_0, label='T3 Aproximated Solution', linestyle='--')
plt.plot(solution1.x, -solution1.x -0.5*np.e**(-solution1.x) + 1.5*np.e**solution1.x, label='True Solution', linestyle='--')
plt.xlabel('x') # Label for the x-axis
plt.ylabel('y') # Label for the y-axis
plt.grid(True) # Show a grid for better readability
plt.legend()
plt.show()
```

## Truncation Error

Truncation error is the error that is introduced by approximating a infinite series, such as the Taylor series we use, with a finite number of terms. I'm suspecting in this case, that $T_1$ introduces the least amount of truncation error at each step and this is why it is the best aproximation of the ODE.

We can define our truncation error ($R$) as the difference be the true value ($Y^*$) and the predicted value ($Y$). In our aproximation, this is defined as:

$$R = Y^*(x+h) - Y(x+h) = Y^*(x+h) - T \cdot S \cdot Y(x) $$

Where $S$ is the stepping matrix and $T$ is the transformation matrix.

Since our approximation is based on the Taylor series, the truncation error is the remaining terms in the series that aren't used. When $k$ terms of the taylor series are used, the truncation error ($r$) is :

$$ r = \sum_{n = k+2}^{\infty}  \frac{\mathbb y^{n}(x)}{n!}\cdot(h)^n$$

Since $y^{n}$ be pulled out as a constant for a given step, the limit of the magnitude of this error can be defined using big-O notation, such that when $k$ terms are used, $r$ is $O(h^{k+3})$.

Looking at just the stepping matrix, we can see the truncation error is:

$$S = \begin{bmatrix}
1 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 0 & 0\\
0 & 0 & 1 & h & \frac{h^2}{2}\\
0 & 0 & 0 & 1 & h\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix} \rightarrow R =  \begin{bmatrix}
0\\
0\\
O(h^3)\\
O(h^2)\\
O(h)\\
\end{bmatrix}$$


Taking it back to the above example of $y'' = x + y$, and the three transformation matrices it defines, we can apply the transformation matrices to the vector $R$ of the standard stepping matrix to get the truncation error for each of the approximations.

$$T_1 \cdot R  = \begin{bmatrix}
0\\
0\\
O(h^3)\\
O(h^2)\\
O(h^3)\\
\end{bmatrix}
$$

$$T_2 \cdot R = \begin{bmatrix}
0\\
O(h)\\
O(h^3)\\
O(h^2)\\
O(h)\\
\end{bmatrix}
$$

$$T_3 \cdot R  = \begin{bmatrix}
0\\
0\\
O(h)\\
O(h^2)\\
O(h)\\
\end{bmatrix}
$$

Now looking at the magnitude of these vectors:


$$ |T_1 \cdot R | = \sqrt{(O(h^3))^2 + (O(h^2))^2 + (O(h^3))^2} = O(h^2) $$
$$ |T_2 \cdot R | = \sqrt{(O(h))^2 + (O(h^3))^2 + (O(h^2))^2 + (O(h))^2}  = O(h) $$
$$ |T_3 \cdot R | = \sqrt{(O(h))^2 + (O(h^2))^2 + (O(h))^2} $ = O(h) $$


This shows that $|T_1 \cdot R |$ shrinks as $h$ gets smaller faster than when you use $T_2$ or $T_3$. 
More generally, you can say that $T_1$ will lead to the least error.

Now these are the errors for each step, so now looking at the truncation error for the whole approximation, we can multiply the error by the number of steps.

Say we want to find the value for for the curve at $x=1$, we would need to take $1/h$ steps to find this value.

$$ |T_1 \cdot R | \cdot \frac{1}{h} = O(h) $$
$$ |T_2 \cdot R | \cdot \frac{1}{h} = O(1) $$
$$ |T_3 \cdot R | \cdot \frac{1}{h} = O(1) $$


We can now test this by running some approximations with various values of h and the different matrices, the results for which are below. We can see that the $T_1$ result follows the $O(h)$ curve while the $T_2$ and $T_3$ result follows the $O(1)$ curve.


```{python, echo = FALSE}
h_list = [1/1, 1/5, 1/10, 1/50, 1/100, 1/500]
t1_list = [] 
t3_list = []
for h in h_list:
    t1_list.append( linear(
        init_y,
        expanded_euler,
        T1,
        steps=int(1/h), h=h).__dict__['y_0'][-1] - (-1 -0.5*np.e**(-1) + 1.5*np.e**1))
        
    t3_list.append( linear(
        init_y,
        expanded_euler,
        T3,
        steps=int(1/h), h=h).__dict__['y_0'][-1]- (-1 -0.5*np.e**(-1) + 1.5*np.e**1))

plt.figure()
plt.loglog(h_list, np.abs(t1_list), marker='o', linestyle='-',label='T1 absolute error')
plt.loglog(h_list, np.abs(t3_list), marker='o', linestyle='-', label='T2 and T3 absolute error')
plt.loglog(h_list, [1 for x in h_list], linestyle='dashed',label='O(1) error')
plt.loglog(h_list, [ h for h in h_list], linestyle='dashed',label='O(h) error')
plt.loglog(h_list, [h**2 for h in h_list], linestyle='dashed',label='O(h^2) error')
plt.xlabel('Step size')
plt.ylabel('Absolute Error')
plt.legend()
plt.grid(True)
plt.show()

```

So now trying to generalise this beyond the example we've worked through, the reduction in error using the $T_1$ matrix was caused by redefining the term with the most truncation error, $y''$ with terms with less truncation error, $y$ and $x$. Both $T_2$ and $T_3$ failed to do this. 

So in general, to minimise error we should try to define the highest derivative in terms of the lower terms, as this will remove the $O(h)$ error from the truncation vector.

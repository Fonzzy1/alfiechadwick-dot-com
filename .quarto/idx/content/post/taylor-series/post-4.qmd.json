{"title":"Optimising the settings for the ODE solver","markdown":{"yaml":{"title":"Optimising the settings for the ODE solver","author":"Alfie Chadwick","date":"2024-3-15","lastmod":"`r Sys.Date()`","tags":["Calculus","Algebra"]},"headingText":"Truncation Error","containsRefs":false,"markdown":"\n\n```{python example-setup}\n#| echo: false\n#| warning: false\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport mplcatppuccin\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nmpl.style.use(\"macchiato\")\n\ndef expanded_euler(dims, h):\n    step_matrix = np.zeros((dims, dims))\n    for i in range(dims):\n        for j in range(i, dims):\n            # Is 1, and h at j-i =0, 1 respectively\n            step_matrix[i, j] = h ** (j - i) / math.factorial(j - i)\n    expanded_matrix = add_x_and_1(step_matrix, h)\n    return expanded_matrix\n\n\ndef add_x_and_1(original_matrix, h):\n    new_size = len(original_matrix) + 2\n    new_matrix = np.zeros((new_size, new_size), dtype=original_matrix.dtype)\n    # Set the 2x2 top left matrix\n    new_matrix[0:2, 0:2] = [[1, 0], [h, 1]]\n    # Copy the original matrix to the bottom right of the new matrix.\n    new_matrix[2:, 2:] = original_matrix\n    return new_matrix\n\n\n\nclass Solution:\n    def __init__(self, input_list: list):\n        solution_list = sorted(input_list, key=lambda x: x[1])\n        dims = len(solution_list[0]) - 2\n        self.x = np.array([x[1] for x in input_list])\n        value_lists = [[] for _ in range(dims)]\n        for v in input_list:\n            for i in range(dims):\n                value_lists[i].append(v[i + 2])\n        for i in range(dims):\n            self.__dict__[f\"y_{i}\"] = np.array(value_lists[i])\n    def interpolate(self, x, y_n):\n        \"\"\"\n        allows you to get any value from the solution by interpolating the points\n\n        \"\"\"\n        y_values = self.__dict__[f\"y_{y_n}\"]\n        x_max_index = np.where(self.x >= x)[0][0]\n        x_min_index = np.where(self.x <= x)[0][-1]\n        x_at_x_max = self.x[x_max_index]\n        x_at_x_min = self.x[x_min_index]\n        y_at_x_max = y_values[x_max_index]\n        y_at_x_min = y_values[x_min_index]\n        slope = (y_at_x_max - y_at_x_min) / (x_at_x_max - x_at_x_min)\n        value = y_at_x_min + slope * (x - x_at_x_min)\n        return value\n\ndef linear(y, step_matrix_generator, transformation_matrix, steps=10, h=0.1):\n    dims = len(y) - 2\n    step_matrix = transformation_matrix @ step_matrix_generator(dims, h)\n    output_list = []\n    y_n = y.copy()\n    i = 0\n    while i < steps:\n        y_n = step_matrix @ y_n\n        output_list.append(y_n)\n        i += 1\n    return Solution(output_list)\n```\n\nIn the [last post](/2024/01/12/making-my-ode-solver-solve-odes/) in this series, I introduced my method for approximating ODEs. But after re-rereading it, I found myself questioning the step where I convert the ODE into a matrix.\n\nThis is best exemplified by the ODE $y'' = x + y$ which goes through the point $y(0) = 1$ and $y'(x) = 1$. Converting it into a matrix, we would define $T$ as:\n\n$$\nT_1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 1 & 1 & 0 & 0\\\\\n\\end{bmatrix}\n$$\n\nHowever, rearranging this ODE into  $x = y'' - y$  or $y = y'' - x$, it would be just as reasonable to define T as:\n\n$$\nT_2 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & -1 & 0 & 1\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n$$\nT_3 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & -1 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\nSo does this matter? Since they are all equaly valid ways of representing the ODE, surley they will all lead to the same conclusion. However Using each of these matrices the ODE we get very different solutions. $T1$ is the closet, $T3$ and $T2$ are both equally far off.\n\n```{python example, echo = FALSE}\n#| echo: false\n#| warning: false\ninit_y = [1,0,1,1,1] #[1,x,y,y']\nT1 = np.array([\n    [1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 1, 1, 0, 0]\n])\n\nT2 = np.array([\n    [1, 0, 0, 0, 0],\n    [0, 0, -1, 0, 1],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 1]\n])\n\nT3 = np.array([\n    [1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, -1, 0, 0, 1],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 1]\n])\nsteps = 10\n\nsolution1 = linear(\n    init_y,\n    expanded_euler,\n    T1,\n    steps=steps, h=1/steps)\n    \nsolution2 = linear(\n    init_y,\n    expanded_euler,\n    T2,\n    steps=2*steps, h=-1/steps)\nsolution3 = linear(\n    init_y,\n    expanded_euler,\n    T3,\n    steps=steps, h=1/steps)\n\n\nplt.plot(solution1.x, solution1.y_0, label='T1 Aproximated Solution')\nplt.plot(solution2.x, solution2.y_0, label='T2 Aproximated Solution')\nplt.plot(solution3.x, solution3.y_0, label='T3 Aproximated Solution', linestyle='--')\nplt.plot(solution1.x, -solution1.x -0.5*np.e**(-solution1.x) + 1.5*np.e**solution1.x, label='True Solution', linestyle='--')\nplt.xlabel('x') # Label for the x-axis\nplt.ylabel('y') # Label for the y-axis\nplt.grid(True) # Show a grid for better readability\nplt.legend()\nplt.show()\n```\n\n\nTruncation error is the error that is introduced by approximating a infinite series, such as the Taylor series we use, with a finite number of terms. I'm suspecting in this case, that $T_1$ introduces the least amount of truncation error at each step and this is why it is the best aproximation of the ODE.\n\nWe can define our truncation error ($R$) as the difference be the true value ($Y^*$) and the predicted value ($Y$). In our aproximation, this is defined as:\n\n$$R = Y^*(x+h) - Y(x+h) = Y^*(x+h) - T \\cdot S \\cdot Y(x) $$\n\nWhere $S$ is the stepping matrix and $T$ is the transformation matrix.\n\nSince our approximation is based on the Taylor series, the truncation error is the remaining terms in the series that aren't used. When $k$ terms of the taylor series are used, the truncation error ($r$) is :\n\n$$ r = \\sum_{n = k+2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n$$\n\nSince $y^{n}$ be pulled out as a constant for a given step, the limit of the magnitude of this error can be defined using big-O notation, such that when $k$ terms are used, $r$ is $O(h^{k+3})$.\n\nLooking at just the stepping matrix, we can see the truncation error is:\n\n$$S = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & h & \\frac{h^2}{2}\\\\\n0 & 0 & 0 & 1 & h\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix} \\rightarrow R =  \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}$$\n\n\nTaking it back to the above example of $y'' = x + y$, and the three transformation matrices it defines, we can apply the transformation matrices to the vector $R$ of the standard stepping matrix to get the truncation error for each of the approximations.\n\n$$T_1 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h^3)\\\\\n\\end{bmatrix}\n$$\n\n$$T_2 \\cdot R = \\begin{bmatrix}\n0\\\\\nO(h)\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n$$\n\n$$T_3 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n$$\n\nNow looking at the magnitude of these vectors:\n\n\n$$ |T_1 \\cdot R | = \\sqrt{(O(h^3))^2 + (O(h^2))^2 + (O(h^3))^2} = O(h^2) $$\n$$ |T_2 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^3))^2 + (O(h^2))^2 + (O(h))^2}  = O(h) $$\n$$ |T_3 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^2))^2 + (O(h))^2} $ = O(h) $$\n\n\nThis shows that $|T_1 \\cdot R |$ shrinks as $h$ gets smaller faster than when you use $T_2$ or $T_3$. \nMore generally, you can say that $T_1$ will lead to the least error.\n\nNow these are the errors for each step, so now looking at the truncation error for the whole approximation, we can multiply the error by the number of steps.\n\nSay we want to find the value for for the curve at $x=1$, we would need to take $1/h$ steps to find this value.\n\n$$ |T_1 \\cdot R | \\cdot \\frac{1}{h} = O(h) $$\n$$ |T_2 \\cdot R | \\cdot \\frac{1}{h} = O(1) $$\n$$ |T_3 \\cdot R | \\cdot \\frac{1}{h} = O(1) $$\n\n\nWe can now test this by running some approximations with various values of h and the different matrices, the results for which are below. We can see that the $T_1$ result follows the $O(h)$ curve while the $T_2$ and $T_3$ result follows the $O(1)$ curve.\n\n\n```{python, echo = FALSE}\n\n#| echo: false\n#| warning: false\nh_list = [1/1, 1/5, 1/10, 1/50, 1/100, 1/500]\nt1_list = [] \nt3_list = []\nfor h in h_list:\n    t1_list.append( linear(\n        init_y,\n        expanded_euler,\n        T1,\n        steps=int(1/h), h=h).__dict__['y_0'][-1] - (-1 -0.5*np.e**(-1) + 1.5*np.e**1))\n        \n    t3_list.append( linear(\n        init_y,\n        expanded_euler,\n        T3,\n        steps=int(1/h), h=h).__dict__['y_0'][-1]- (-1 -0.5*np.e**(-1) + 1.5*np.e**1))\n\nplt.figure()\nplt.loglog(h_list, np.abs(t1_list), marker='o', linestyle='-',label='T1 absolute error')\nplt.loglog(h_list, np.abs(t3_list), marker='o', linestyle='-', label='T2 and T3 absolute error')\nplt.loglog(h_list, [1 for x in h_list], linestyle='dashed',label='O(1) error')\nplt.loglog(h_list, [ h for h in h_list], linestyle='dashed',label='O(h) error')\nplt.loglog(h_list, [h**2 for h in h_list], linestyle='dashed',label='O(h^2) error')\nplt.xlabel('Step size')\nplt.ylabel('Absolute Error')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n```\n\nSo now trying to generalise this beyond the example we've worked through, the reduction in error using the $T_1$ matrix was caused by redefining the term with the most truncation error, $y''$ with terms with less truncation error, $y$ and $x$. Both $T_2$ and $T_3$ failed to do this. \n\nSo in general, to minimise error we should try to define the highest derivative in terms of the lower terms, as this will remove the $O(h)$ error from the truncation vector.\n","srcMarkdownNoYaml":"\n\n```{python example-setup}\n#| echo: false\n#| warning: false\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport mplcatppuccin\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nmpl.style.use(\"macchiato\")\n\ndef expanded_euler(dims, h):\n    step_matrix = np.zeros((dims, dims))\n    for i in range(dims):\n        for j in range(i, dims):\n            # Is 1, and h at j-i =0, 1 respectively\n            step_matrix[i, j] = h ** (j - i) / math.factorial(j - i)\n    expanded_matrix = add_x_and_1(step_matrix, h)\n    return expanded_matrix\n\n\ndef add_x_and_1(original_matrix, h):\n    new_size = len(original_matrix) + 2\n    new_matrix = np.zeros((new_size, new_size), dtype=original_matrix.dtype)\n    # Set the 2x2 top left matrix\n    new_matrix[0:2, 0:2] = [[1, 0], [h, 1]]\n    # Copy the original matrix to the bottom right of the new matrix.\n    new_matrix[2:, 2:] = original_matrix\n    return new_matrix\n\n\n\nclass Solution:\n    def __init__(self, input_list: list):\n        solution_list = sorted(input_list, key=lambda x: x[1])\n        dims = len(solution_list[0]) - 2\n        self.x = np.array([x[1] for x in input_list])\n        value_lists = [[] for _ in range(dims)]\n        for v in input_list:\n            for i in range(dims):\n                value_lists[i].append(v[i + 2])\n        for i in range(dims):\n            self.__dict__[f\"y_{i}\"] = np.array(value_lists[i])\n    def interpolate(self, x, y_n):\n        \"\"\"\n        allows you to get any value from the solution by interpolating the points\n\n        \"\"\"\n        y_values = self.__dict__[f\"y_{y_n}\"]\n        x_max_index = np.where(self.x >= x)[0][0]\n        x_min_index = np.where(self.x <= x)[0][-1]\n        x_at_x_max = self.x[x_max_index]\n        x_at_x_min = self.x[x_min_index]\n        y_at_x_max = y_values[x_max_index]\n        y_at_x_min = y_values[x_min_index]\n        slope = (y_at_x_max - y_at_x_min) / (x_at_x_max - x_at_x_min)\n        value = y_at_x_min + slope * (x - x_at_x_min)\n        return value\n\ndef linear(y, step_matrix_generator, transformation_matrix, steps=10, h=0.1):\n    dims = len(y) - 2\n    step_matrix = transformation_matrix @ step_matrix_generator(dims, h)\n    output_list = []\n    y_n = y.copy()\n    i = 0\n    while i < steps:\n        y_n = step_matrix @ y_n\n        output_list.append(y_n)\n        i += 1\n    return Solution(output_list)\n```\n\nIn the [last post](/2024/01/12/making-my-ode-solver-solve-odes/) in this series, I introduced my method for approximating ODEs. But after re-rereading it, I found myself questioning the step where I convert the ODE into a matrix.\n\nThis is best exemplified by the ODE $y'' = x + y$ which goes through the point $y(0) = 1$ and $y'(x) = 1$. Converting it into a matrix, we would define $T$ as:\n\n$$\nT_1 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 1 & 1 & 0 & 0\\\\\n\\end{bmatrix}\n$$\n\nHowever, rearranging this ODE into  $x = y'' - y$  or $y = y'' - x$, it would be just as reasonable to define T as:\n\n$$\nT_2 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & -1 & 0 & 1\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n$$\nT_3 = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & -1 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix}\n$$\n\nSo does this matter? Since they are all equaly valid ways of representing the ODE, surley they will all lead to the same conclusion. However Using each of these matrices the ODE we get very different solutions. $T1$ is the closet, $T3$ and $T2$ are both equally far off.\n\n```{python example, echo = FALSE}\n#| echo: false\n#| warning: false\ninit_y = [1,0,1,1,1] #[1,x,y,y']\nT1 = np.array([\n    [1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 1, 1, 0, 0]\n])\n\nT2 = np.array([\n    [1, 0, 0, 0, 0],\n    [0, 0, -1, 0, 1],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 1]\n])\n\nT3 = np.array([\n    [1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, -1, 0, 0, 1],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 1]\n])\nsteps = 10\n\nsolution1 = linear(\n    init_y,\n    expanded_euler,\n    T1,\n    steps=steps, h=1/steps)\n    \nsolution2 = linear(\n    init_y,\n    expanded_euler,\n    T2,\n    steps=2*steps, h=-1/steps)\nsolution3 = linear(\n    init_y,\n    expanded_euler,\n    T3,\n    steps=steps, h=1/steps)\n\n\nplt.plot(solution1.x, solution1.y_0, label='T1 Aproximated Solution')\nplt.plot(solution2.x, solution2.y_0, label='T2 Aproximated Solution')\nplt.plot(solution3.x, solution3.y_0, label='T3 Aproximated Solution', linestyle='--')\nplt.plot(solution1.x, -solution1.x -0.5*np.e**(-solution1.x) + 1.5*np.e**solution1.x, label='True Solution', linestyle='--')\nplt.xlabel('x') # Label for the x-axis\nplt.ylabel('y') # Label for the y-axis\nplt.grid(True) # Show a grid for better readability\nplt.legend()\nplt.show()\n```\n\n## Truncation Error\n\nTruncation error is the error that is introduced by approximating a infinite series, such as the Taylor series we use, with a finite number of terms. I'm suspecting in this case, that $T_1$ introduces the least amount of truncation error at each step and this is why it is the best aproximation of the ODE.\n\nWe can define our truncation error ($R$) as the difference be the true value ($Y^*$) and the predicted value ($Y$). In our aproximation, this is defined as:\n\n$$R = Y^*(x+h) - Y(x+h) = Y^*(x+h) - T \\cdot S \\cdot Y(x) $$\n\nWhere $S$ is the stepping matrix and $T$ is the transformation matrix.\n\nSince our approximation is based on the Taylor series, the truncation error is the remaining terms in the series that aren't used. When $k$ terms of the taylor series are used, the truncation error ($r$) is :\n\n$$ r = \\sum_{n = k+2}^{\\infty}  \\frac{\\mathbb y^{n}(x)}{n!}\\cdot(h)^n$$\n\nSince $y^{n}$ be pulled out as a constant for a given step, the limit of the magnitude of this error can be defined using big-O notation, such that when $k$ terms are used, $r$ is $O(h^{k+3})$.\n\nLooking at just the stepping matrix, we can see the truncation error is:\n\n$$S = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & h & \\frac{h^2}{2}\\\\\n0 & 0 & 0 & 1 & h\\\\\n0 & 0 & 0 & 0 & 1\\\\\n\\end{bmatrix} \\rightarrow R =  \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}$$\n\n\nTaking it back to the above example of $y'' = x + y$, and the three transformation matrices it defines, we can apply the transformation matrices to the vector $R$ of the standard stepping matrix to get the truncation error for each of the approximations.\n\n$$T_1 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h^3)\\\\\n\\end{bmatrix}\n$$\n\n$$T_2 \\cdot R = \\begin{bmatrix}\n0\\\\\nO(h)\\\\\nO(h^3)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n$$\n\n$$T_3 \\cdot R  = \\begin{bmatrix}\n0\\\\\n0\\\\\nO(h)\\\\\nO(h^2)\\\\\nO(h)\\\\\n\\end{bmatrix}\n$$\n\nNow looking at the magnitude of these vectors:\n\n\n$$ |T_1 \\cdot R | = \\sqrt{(O(h^3))^2 + (O(h^2))^2 + (O(h^3))^2} = O(h^2) $$\n$$ |T_2 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^3))^2 + (O(h^2))^2 + (O(h))^2}  = O(h) $$\n$$ |T_3 \\cdot R | = \\sqrt{(O(h))^2 + (O(h^2))^2 + (O(h))^2} $ = O(h) $$\n\n\nThis shows that $|T_1 \\cdot R |$ shrinks as $h$ gets smaller faster than when you use $T_2$ or $T_3$. \nMore generally, you can say that $T_1$ will lead to the least error.\n\nNow these are the errors for each step, so now looking at the truncation error for the whole approximation, we can multiply the error by the number of steps.\n\nSay we want to find the value for for the curve at $x=1$, we would need to take $1/h$ steps to find this value.\n\n$$ |T_1 \\cdot R | \\cdot \\frac{1}{h} = O(h) $$\n$$ |T_2 \\cdot R | \\cdot \\frac{1}{h} = O(1) $$\n$$ |T_3 \\cdot R | \\cdot \\frac{1}{h} = O(1) $$\n\n\nWe can now test this by running some approximations with various values of h and the different matrices, the results for which are below. We can see that the $T_1$ result follows the $O(h)$ curve while the $T_2$ and $T_3$ result follows the $O(1)$ curve.\n\n\n```{python, echo = FALSE}\n\n#| echo: false\n#| warning: false\nh_list = [1/1, 1/5, 1/10, 1/50, 1/100, 1/500]\nt1_list = [] \nt3_list = []\nfor h in h_list:\n    t1_list.append( linear(\n        init_y,\n        expanded_euler,\n        T1,\n        steps=int(1/h), h=h).__dict__['y_0'][-1] - (-1 -0.5*np.e**(-1) + 1.5*np.e**1))\n        \n    t3_list.append( linear(\n        init_y,\n        expanded_euler,\n        T3,\n        steps=int(1/h), h=h).__dict__['y_0'][-1]- (-1 -0.5*np.e**(-1) + 1.5*np.e**1))\n\nplt.figure()\nplt.loglog(h_list, np.abs(t1_list), marker='o', linestyle='-',label='T1 absolute error')\nplt.loglog(h_list, np.abs(t3_list), marker='o', linestyle='-', label='T2 and T3 absolute error')\nplt.loglog(h_list, [1 for x in h_list], linestyle='dashed',label='O(1) error')\nplt.loglog(h_list, [ h for h in h_list], linestyle='dashed',label='O(h) error')\nplt.loglog(h_list, [h**2 for h in h_list], linestyle='dashed',label='O(h^2) error')\nplt.xlabel('Step size')\nplt.ylabel('Absolute Error')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n```\n\nSo now trying to generalise this beyond the example we've worked through, the reduction in error using the $T_1$ matrix was caused by redefining the term with the most truncation error, $y''$ with terms with less truncation error, $y$ and $x$. Both $T_2$ and $T_3$ failed to do this. \n\nSo in general, to minimise error we should try to define the highest derivative in terms of the lower terms, as this will remove the $O(h)$ error from the truncation vector.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"../../../themes/code_block.css","output-file":"post-4.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","theme":"../../../themes/theme.scss","title":"Optimising the settings for the ODE solver","author":"Alfie Chadwick","date":"2024-3-15","lastmod":"`r Sys.Date()`","tags":["Calculus","Algebra"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}